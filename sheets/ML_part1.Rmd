---
title: "Hands-on Machine Learning with R - Module 1"
author: "Katrien Antonio & Roel Henckaerts"
date: '[hands-on-machine-learning-R-module-1](https://github.com/katrienantonio/hands-on-machine-learning-R-module-1) | December 10 & 17, 2020'
output:
  xaringan::moon_reader:
    css:
    - default
    - css/metropolis.css
    - css/metropolis-fonts.css
    - css/my-css.css
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      highlightLines: yes
      countIncrementalSlides: no
      highlightSpans: yes
  html_document:
    df_print: paged
subtitle: Hands-on webinar  <html><div style='float:left'></div><hr
  align='center' color='#116E8A' size=1px width=97%></html>
graphics: yes
editor_options:
  chunk_output_type: console
header-includes:
- \usepackage{tikz}
- \usetikzlibrary{shapes.geometric,shapes, snakes, arrows}
- \usepackage{amsfonts}
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{color}
- \usepackage{graphicx}
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
# options(knitr.table.format = "html")
library(tidyverse)
library(fontawesome) # from github: https://github.com/rstudio/fontawesome
library(DiagrammeR)
library(emo) # from github: https://github.com/hadley/emo
library(gt) # from github: https://github.com/rstudio/gt
library(countdown) # from github: https://github.com/gadenbuie/countdown 
library(here)
```

```{r setup_greenwell, include=FALSE}
# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE, 
        crayon.enabled = TRUE)

# Set global knitr chunk options
knitr::opts_chunk$set(
  dev = "svg",
  fig.align = "center",
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE 
)

# colors - I copied most of these from # https://github.com/edrubin/EC524W20
dark2 <- RColorBrewer::brewer.pal(8, name = "Dark2")
KULbg <- "#116E8A"
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#3b3b9a"
green      = "#8bb174"
grey_light = "grey70"
grey_mid   = "grey50"
grey_dark  = "grey20"
purple     = "#6A5ACD"
slate      = "#314f4f"
```

class: inverse, center, middle
name: prologue

# Prologue

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

name: introduction

# Introduction

### Course

`r fa(name = "github", fill = KULbg)` https://github.com/katrienantonio/hands-on-machine-learning-R-module-1

The course repo on GitHub, where you can find the data sets, lecture sheets, R scripts and R markdown files.

--

### Us

`r fa(name = "link", fill = KULbg)` [https://katrienantonio.github.io/](https://katrienantonio.github.io/) & [https://henckr.github.io/](https://henckr.github.io/)

`r fa(name = "paper-plane", fill = KULbg)` [katrien.antonio@kuleuven.be](mailto:katrien.antonio@kuleuven.be) & [roel.henckaerts@kuleuven.be](mailto:roel.henckaerts@kuleuven.be)

`r fa('graduation-cap', fill = KULbg)` (Katrien) Professor in insurance data science

`r fa('graduation-cap', fill = KULbg)` (Roel) PhD student in insurance data science

---

name: checklist

# Checklist

☑ Do you have a fairly recent version of R?
  ```{r eval=TRUE}
  version$version.string
  ```

☑ Do you have a fairly recent version of RStudio? 
  ```{r eval=FALSE}
  RStudio.Version()$version
  ## Requires an interactive session but should return something like "[1] ‘1.3.1093’"
  ```

☑ Have you installed the R packages listed in the software requirements? 

or

☑ Have you created an account on RStudio Cloud (to avoid any local installation issues)?
  
---

name: why-this-course # inspired by Grant McDermott intro lecture

# Why this course?

### The goals of this course .font140[`r fa(name = "fas fa-rocket", fill = KULbg)`]

--

* develop practical .KULbginline[machine learning (ML) foundations]

--

* .KULbginline[fill in the gaps] left by traditional training in actuarial science or econometrics

--

* focus on the use of ML methods for the .KULbginline[analysis of frequency + severity data], but also .KULbginline[non-standard data] such as images 

--

* .KULbginline[explore] a substantial range of .KULbginline[methods (and data types)] (from GLMs to deep learning), but - most importantly - .KULbginline[build foundation] so that you can explore other methods (and data types) yourself. 

--

<br>

> *"In short, we will cover things that we wish someone had taught us in our undergraduate programs."* 
> <br>
> .font80[This quote is from the [Data science for economists course](http://github.com/uo-ec607/lectures) by Grant McDermott.]

---

# Module 1's Outline

.pull-left[

* [Prologue](#prologue)

* [Knowing me, knowing you: <br> statistical and machine learning](#knowing)

  - Supervised and unsupervised learning
  - Regression and classification
  - Statistical modeling: the two cultures

* [Model accuracy and loss functions](#accuracy)

* [Overfitting and bias-variance tradeoff](#overfit)

* [Data splitting, Resampling methods](#resampling)

]

.pull-right[

* [Parameter tuning](#tuning) 

  - with {caret}, {rsample} and {purrr}

* [Target and feature engineering](#engineering)

  - Data leakage
  - Pre-processing steps
  - Specifying blue-prints with {recipes}
  - Putting it all together: {recipes} and {caret}/{rsample}

* [Regression models](#regression)

  - Creating models in R and tidy model output with {broom}
  - GLMs with {glm}
  - GAMs with {mgcv}
  - Regularized (G)LMs with {glmnet}.

]


---

name: map-ML-world
class: right, middle, clear
background-image: url("img/map_ML_world.jpg")
background-size: 45% 
background-position: left


.KULbginline[Some roadmaps to explore the ML landscape...] 

<img src = "img/AI_ML_DL.jpg" height = "350px" />

.font60[Source: [Machine Learning for Everyone In simple words. With real-world examples. Yes, again.](https://vas3k.com/blog/machine_learning/)]


---

name: map-ML-world
class: right, middle, clear
background-image: url("img/main_types_ML.jpg")
background-size: 85% 
background-position: middle

---

class: inverse, center, middle
name: knowing

# Knowing me, knowing you: 
<br> <br>
# statistical and machine learning 

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

name: supervised-learning

# Supervised learning

.pull-left-alt[

Supervised learning builds ("learns") a .blue[model] $\color{#3b3b9a}{f}$ (*the Signal*) such that the .orange[outcome or target] $\color{#fb6107}{Y}$ can be written as

$$\color{#FFA500}{Y} = \color{#3b3b9a}{f}(\color{#e64173}{x_1, \ldots, x_p}) + \epsilon$$
with .pink[features] $\color{#e64173}{x_1, \ldots, x_p}$ and error term $\epsilon$ (*the Noise*).

Supervised learners construct .KULbginline[predictive models].

<br> <br>

.footnote[Picture taken from [Machine Learning for Everyone. In simple words. With real-world examples. Yes, again](https://vas3k.com/blog/machine_learning/)]

]

.pull-right-alt[

.center[
```{r out.width = '100%', echo=FALSE}
knitr::include_graphics("img/supervised_unsupervised_drawing.jpg")
```
]




]
---



name: unsupervised-learning

# Unsupervised learning

.pull-left-alt[

With unsupervised learning there is .KULbginline[NO] .orange[outcome or target] $\color{#FFA500}{Y}$, only the feature vector $\color{#e64173}{x = (x_1, \ldots, x_p)}$. 

Let $n$ denote the sample size and $p$ the number of features. 

Then, $\color{#e64173}{X}$ is the $n \times p$ matrix of features, with $\color{#e64173}{x}_{i,j}$ observation $i$ on variable or feature $j$.

Unsupervised learners construct .KULbginline[descriptive models], without any *supervising* output, letting the data "speak for itself".


]

.pull-right-alt[

.center[
```{r out.width = '80%', out.height = '55%', echo=FALSE}
knitr::include_graphics("img/K-means_drawing.jpg")
```
]

.footnote[Picture taken from [Machine Learning for Everyone. In simple words. With real-world examples. Yes, again](https://vas3k.com/blog/machine_learning/)]


]
---

class: clear

<br> <br>

.center[
```{r out.width = '90%', echo=FALSE}
knitr::include_graphics("img/supervised_unsupervised_robot.jpg")
```
]

.footnote[Picture taken from [this source](https://twitter.com/athena_schools/status/1063013435779223553).]

---

name: what's-in-a-name

# What's in a name?

.KULbginline[Machine learning] constructs algorithms that learn from data. 

--

.KULbginline[Statistical learning] emphasizes statistical models and the assessment of uncertainty.

--

.KULbginline[Data science] applies mathematics, statistics, machine learning, engineering, etc. to extract knowledge form data.
--

> *"Data Science is statistics on a Mac `r fa(name = "apple", fill = KULbg)`. "*

.center[
<img src="img/ElementsStatLearning.png" alt="Drawing" style="height: 250px;"/>  &nbsp;&nbsp;&nbsp; <img src="img/ISL.png" alt="Drawing" style="height: 250px;"/> 
&nbsp;&nbsp;&nbsp; <img src="img/AppliedPredMod.png" alt="Drawing" style="height: 250px;"/> &nbsp;&nbsp;&nbsp; <img src="img/boehmke_greenwell.jpg" alt="Drawing" style="height: 250px;"/> &nbsp;&nbsp;&nbsp; <img src="img/molnar.png" alt="Drawing" style="height: 250px;"/>
]

Source: Brandon M. Greenwell on [Introduction to Machine Learning in `r fa(name = "r-project", fill = "#F92672")`](https://github.com/bgreenwell/intro-ml-r).

---

name: two-cultures

# Statistical modeling: the two cultures

Consider a vector of input variables $\color{#e64173}{x}$, being transformed into some vector of response variables $\color{#FFA500}{y}$ via a black box algorithm. 

.center[
<img src="img/Breiman_nature.png" alt="Drawing" style="width: 300px;"/>  
]

--

.pull-left[

.KULbginline[Statistical learning or data modeling culture]

* assume statistical model, estimate parameter values
* validate with goodness-of-fit tests and residual inspection

.center[
<img src="img/Breiman_data_modeling.png" alt="Drawing" style="width: 300px;"/>  
]
]
--

.pull-right[

.KULbginline[Machine learning or algo modeling culture]

* inside of the box is complex and unknown
* find algorithm $\color{#3b3b9a}{f}(\color{#e64173}{x})$ to predict $\color{#FFA500}{y}$
* measure performance by predictive accuracy

.center[
<img src="img/Breiman_algo_modeling.png" alt="Drawing" style="width: 300px;"/>  
]

]

Source: Breiman (2001, Statistical Science) on *Statistical modeling: the two cultures.*
---

name: statistical-machine-learning

# Newspeak from the two cultures

<br> 
<br>

| Statistical learning           |  Machine learning
:------:|:-------------------------:|:-------------------------:
.KULbginline[origin] | statistics | computer science 
.KULbginline[*f(x)*] | model | algorithm
.KULbginline[emphasis] | interpretability, precision and uncertainty | large scale applicability, prediction accuracy
.KULbginline[jargon] | parameters, estimation | weights, learning
.KULbginline[CI] | uncertainty of parameters | no notion of uncertainty 
.KULbginline[assumptions] | explicit a priori assumption | no prior assumption, learn from the data

<br>

Source: read the blog [Why a mathematician, statistician and machine learner solve the same problem differently](https://blog.galvanize.com/why-a-mathematician-statistician-machine-learner-solve-the-same-problem-differently-2/)


---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

As discussed in the lecture, many problems in ML can be approached as a .KULbginline[regression], .KULbginline[classification] or .KULbginline[clustering] problem. 

<br>

.hi-pink[Q]: consider the following .hi-pink[three problem settings] and .hi-pink[label them] as regression, classification or clustering.

<br>

1. In disability insurance: how do disability rates depend on the state of the economy (e.g. GDP)?

2. In MTPL insurance: predict whether a claim is attritional or large, *in casu* a claim that exceeds the threshold of 100 000 EUR?

3. How can we group customers based on the insurance products they bought from the company? 

]


---

class: inverse, center, middle
name: accuracy

# Model accuracy and loss functions

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# Predictive modeling

How to use the observed data to learn or to estimate the unknown $\color{#3b3b9a}{f}(.)$?

$$\begin{eqnarray*}
\color{#FFA500}{y} &=& \color{#3b3b9a}{f}(\color{#e64173}{x_1,x_2,\ldots,x_p})+\epsilon.
\end{eqnarray*}$$

--

How do I .KULbginline[estimate] $\color{#3b3b9a}{f}(.)$ - one way to phrase *all questions* that underly statistical & machine learning.

--

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] &nbsp; - &nbsp; main reasons we want to .KULbginline[learn about] $\color{#3b3b9a}{f}(.)$ 

--

.pull-left[
.font120[.KULbginline[prediction]] 
<br>
predict the target $\color{#FFA500}{y}$ as $\hat{\color{#3b3b9a}{f}}(\color{#e64173}{x})$
<br>
.font140[`r fa(name = "fas fa-box", fill = KULbg)`] - as black box setting? 
<br> <br>
.font120[.KULbginline[inference]]
<br>
how does target $\color{#FFA500}{y}$ depend on features $\color{#e64173}{x}$? 
<br>
.font140[`r fa(name = "fas fa-box-open", fill = KULbg)`] - as white box setting? 
]
--
.pull-right[

```{r fig.align = 'center', fig.width = 8, fig.height = 7, echo = FALSE}
knitr::include_graphics("img/prediction_inference.png")
```
]


---

name: prediction-error

# Prediction errors

Why we're stuck with .KULbginline[irreducible error]

assume $\hat{\color{#3b3b9a}{f}}$ and $\color{#e64173}{x}$ given, then

$$
\begin{aligned}
  \mathop{E}\left[ \left\{ \color{#FFA500}{y} - \hat{\color{#FFA500}{y}} \right\}^2 \right]
  &=
  \mathop{E}\left[ \left\{ \color{#3b3b9a}{f}(\color{#e64173}{x}) + \epsilon - \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}) \right\}^2 \right] \\
  &= \underbrace{\left[ \color{#3b3b9a}{f}(\color{#e64173}{x}) - \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}) \right]^2}_{\text{Reducible}} + \underbrace{\mathop{\text{Var}} \left( \color{#e64173}{\epsilon} \right)}_{\text{Irreducible}}
\end{aligned}
$$
In .KULbginline[less math]:

- if $\epsilon$ exists, then $\color{#e64173}{x}$ cannot perfectly explain $\color{#FFA500}{y}$

- so even if $\hat{\color{#3b3b9a}{f}} = \color{#3b3b9a}{f}$, we still have irreducible error.

--

Thus, to form our .KULbginline[best predictors], we will .KULbginline[minimize reducible error].

---

name: model-accuracy

# Model accuracy

We assess .KULbginline[model] or .KULbginline[predictive accuracy] by 
evaluating how well predictions actually match observed data.

--

Use .KULbginline[loss functions], i.e. metrics that compare predicted values to actual values.

--

.pull-left[.KULbginline[Regression], use e.g. the .hi-pink[Mean Squared Error (MSE)]

$$\begin{eqnarray*}
\frac{1}{n} \sum_{i=1}^n (\color{#FFA500}{y}_i - \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_i))^2,
\end{eqnarray*}$$

Recall: $\color{#FFA500}{y}_i - \hat{\color{#FFA500}{y}}_i = \color{#FFA500}{y}_i - \hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_i)$ is the prediction error.

Objective `r fa(name = "fas fa-bullseye", fill = KULbg)` : minimize!]

--

.pull-right[.KULbginline[Classification], use e.g. the .hi-pink[cross-entropy] or .hi-pink[log loss]
$$\begin{eqnarray*}
-\frac{1}{n} \sum_{i=1}^n \left(\color{#FFA500}{y}_i \cdot \log{(p_i)} + (1-\color{#FFA500}{y}_i) \cdot \log{(1-p_i)}\right).
\end{eqnarray*}$$
 
Objective `r fa(name = "fas fa-bullseye", fill = KULbg)` : minimize!

]

--

<br>

.KULbginline[Many other useful loss functions] (e.g. deviance in regression, Gini index in classification).

.font140[.KULbginline[Take-away]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] &nbsp; - &nbsp; a loss function emphasizes certain types of errors over others `r fa(name = "fas fa-long-arrow-alt-right", fill = KULbg)` pick a meaningful one!


---

class: inverse, center, middle
name: overfit

# Overfitting and bias-variance trade off

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>


---

# Overfitting

The .KULbginline[Signal and the Noise] discussion!

--

Which of the following three models (in green-blue-ish) will best generalize to new data? 

```{r overfitting-linear-regression, echo=FALSE, fig.width=7, fig.height=6/3, out.width = "100%"}
# Simulate some data 
n <- 100
set.seed(8451)
KULbg <- "#116E8A"
df <- tibble::tibble(
  x = runif(n, min = -2, max = 2),
  y = rnorm(n, mean = 1 + 2*x + x^2, sd = 1)
)
p <- ggplot(df, aes(x, y)) + 
  geom_point(alpha = 0.3) +  
  theme_bw()
p1 <- p + 
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, size = 1.5, color = KULbg) +
  ggtitle("Underfitting")
p2 <- p + 
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), se = FALSE, size = 1.5, color = KULbg) +
  ggtitle("Just right?")
p3 <- p + 
  geom_smooth(method = "loess", span = 0.1, se = FALSE, size = 1.5, color = KULbg) +
  ggtitle("Overfitting")
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

.footnote[Inspired by Brandon Greenwell's [Introduction to Machine Learning in `r fa(name = "r-project", fill = "#F92672")`](https://github.com/bgreenwell/intro-ml-r).]

---

# Overfitting (cont.)

With a small training error, but large test error, the model is .KULbginline[overfitting] or working too hard!

--

The expected value of the .hi-pink[test MSE]:

$$\begin{eqnarray*}
    E\left(\color{#FFA500}{y}_0-\hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_0)\right)^2 &=& \text{Var}(\hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_0))+[\text{Bias}(\hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_0))]^2+\text{Var}(\epsilon).
    \end{eqnarray*}$$

--

.font140[.KULbginline[In general]] - with more flexible methods

* variance .font140[`r fa(name = "arrow-circle-up", fill = KULbg)`] and bias .font140[`r fa(name = "arrow-circle-down", fill = KULbg)`]

* their relative rate of change determines whether the test error increases or decreases

--

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`]

* U-shape curves of .hi-pink[test MSE] w.r.t model flexibility

* the .hi-pink[bias-variance tradeoff] is central to quality prediction.

---

# Bias-variance trade off

<br> 

.center[
<img src="img/bias_variance_trade_off.png" alt="Drawing" style="width: 600px;"/>  
]

Source: James et al. (2013) on [http://faculty.marshall.usc.edu/gareth-james/ISL/](An introduction to statistical learning).

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Data are generated from: $\color{#FFA500}{y} = \color{#3b3b9a}{f}(\color{#e64173}{x})+\epsilon$, with the black curve as the true $\color{#3b3b9a}{f}$. The orange (linear regression), blue (smoothing splines) and green (smoothing splines) curves are three estimates for $\color{#3b3b9a}{f}$, with increasing level of complexity.

.hi-pink[Q]: which model do you prefer (orange, blue, green) for each of the following examples? Why? 

.center[
<img src="img/2.9 ISL.png" alt="Drawing" style="width: 500px;"/> 
]

.footnote[Example from James et al. (2013) on [http://faculty.marshall.usc.edu/gareth-james/ISL/](An introduction to statistical learning).]

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Data are generated from: $\color{#FFA500}{y} = \color{#3b3b9a}{f}(\color{#e64173}{x})+\epsilon$, with the black curve as the true $\color{#3b3b9a}{f}$. The orange (linear regression), blue (smoothing splines) and green (smoothing splines) curves are three estimates for $\color{#3b3b9a}{f}$, with increasing level of complexity.

.hi-pink[Q]: which model do you prefer (orange, blue, green) for each of the following examples? Why? 

.center[
<img src="img/2.10 ISL.png" alt="Drawing" style="width: 500px;"/> 
]

.footnote[Example from James et al. (2013) on [http://faculty.marshall.usc.edu/gareth-james/ISL/](An introduction to statistical learning).]

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Data are generated from: $\color{#FFA500}{y} = \color{#3b3b9a}{f}(\color{#e64173}{x})+\epsilon$, with the black curve as the true $\color{#3b3b9a}{f}$. The orange (linear regression), blue (smoothing splines) and green (smoothing splines) curves are three estimates for $\color{#3b3b9a}{f}$, with increasing level of complexity.

.hi-pink[Q]: which model do you prefer (orange, blue, green) for each of the following examples? Why? 

.center[
<img src="img/2.11 ISL.png" alt="Drawing" style="width: 500px;"/> 
]

.footnote[Example from James et al. (2013) on [http://faculty.marshall.usc.edu/gareth-james/ISL/](An introduction to statistical learning).]

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

.font120[The *K*-nearest neighbors (KNN) classifier]

- take the *K* observations in the training data set that are 'closest' to test observation $\color{#e64173}{x}_0$, calculate 


$$\begin{eqnarray*}
\text{Pr}(\color{#FFA500}{Y}=j|\color{#e64173}{X} = \color{#e64173}{x}_0) &=& \frac{1}{K} \sum_{i \in \mathcal{N}_0} \mathbb{I}(\color{#FFA500}{y}_i=j).
\end{eqnarray*}$$

- KNN then assigns the test observation $\color{#e64173}{x}_0$ to the class $j$ with the highest probability, e.g. with *K=3* (from James et al., 2013)

.center[
<img src="img/2.14 ISL.png" alt="Drawing" style="width: 300px;"/> 
]

.hi-pink[Q]: is KNN a supervised learning or unsupervised learning method? Discuss.

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

.font120[The *K*-nearest neighbors (KNN) classifier (cont.)]

Now compare KNN with *K* equals 1, 10 and 100.

.center[
<img src="img/KNN_K_1.png" alt="Drawing" style="height: 250px;"/> 
&nbsp;&nbsp;&nbsp; <img src="img/KNN_K_10.png" alt="Drawing" style="height: 250px;"/> 
&nbsp;&nbsp;&nbsp; <img src="img/KNN_K_100.png" alt="Drawing" style="height: 250px;"/> 
]

.hi-pink[Q]: which classifier do you prefer? Which of these classifiers is under-fitting, which one is over-fitting?

]

---

class: inverse, center, middle
name: resampling

# Data splitting and resampling methods <br> with {caret} and {rsample}

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>


---

name: ames

# Ames Iowa housing data

We will use the Ames Iowa housing data. There are 2,930 properties in the data set. 

The `Sale_Price` (target or response) was recorded along with 80 predictors, including:

* location (e.g. neighborhood) and lot information
* house components (garage, fireplace, pool, porch, etc.)
* general assessments such as overall quality and condition
* number of bedrooms, baths, and so on. 

More details in [De Cock (2011, Journal of Statistics Education)](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf).

The raw data are at [`http://bit.ly/2whgsQM`](http://bit.ly/2whgsQM) but we will use a processed version found in the [`AmesHousing`](https://github.com/topepo/AmesHousing) package. 

You will load the data with the `make_ames()` function from the `AmesHousing` library, and store the data in the object `ames`:

```{r, eval = TRUE}
ames <- AmesHousing::make_ames()
```

---

name: data-splitting

# Data splitting

We fit our model on past data $\{(\color{#e64173}{x}_1,\color{#FFA500}{y}_1),(\color{#e64173}{x}_2,\color{#FFA500}{y}_2),\ldots, (\color{#e64173}{x}_n,\color{#FFA500}{y}_n)\}$
and get $\hat{\color{#3b3b9a}{f}}$. 

*What we want*: how does our model .KULbginline[generalize] to new, unseen data $(\color{#e64173}{x}_0,\color{#FFA500}{y}_0)$, or: &nbsp; is $\hat{\color{#3b3b9a}{f}}(\color{#e64173}{x}_0)$ close to $\color{#FFA500}{y}_0$?

.left-column[

.KULbginline[Training set]

* to develop, to train, to tune, to compare different settings, ...

.KULbginline[Test set]

* to obtain unbiased estimate of final model's performance.
]

.right-column[

```{r fig.align = 'center', out.width = '70%', echo=FALSE}
knitr::include_graphics("img/data_splitting.png")
```

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .footnote[Picture taken from [Introduction to Machine Learning in `r fa(name = "r-project", fill = "#F92672")`](https://github.com/bgreenwell/intro-ml-r).]


]
---

# Data splitting in base

We first demonstrate the splitting of the `ames` housing data into a training and test set, using `base` R instructions. 

.pull-left[

```{r eval = FALSE}
set.seed(123) #<< 
index_1 <- sample(1 : nrow(ames), 
                  size = round(nrow(ames) * 0.7))  
train_1 <- ames[index_1, ]   
test_1  <- ames[-index_1, ]  

nrow(train_1)/nrow(ames)
```

]

.pull-right[

Use `set.seed()` for reproducibility.

]

---

# Data splitting in base

We first demonstrate the splitting of the `ames` housing data into a training and test set, using `base` R instructions. 

.pull-left[

```{r eval = FALSE}
set.seed(123) 
index_1 <- sample(1 : nrow(ames), #<< 
                  size = round(nrow(ames) * 0.7))  #<< 
train_1 <- ames[index_1, ]   
test_1  <- ames[-index_1, ]  

nrow(train_1)/nrow(ames)
```

]

.pull-right[

Sample indices from `1 : nrow(ames)` such that in total 70% of the records is selected.

Vector `index_1` now stores the row numbers of the selected records.

]

---

# Data splitting in base

We first demonstrate the splitting of the `ames` housing data into a training and test set, using `base` R instructions. 

.pull-left[

```{r eval = FALSE}
set.seed(123) 
index_1 <- sample(1 : nrow(ames), 
                  size = round(nrow(ames) * 0.7))   
train_1 <- ames[index_1, ]   #<<
test_1  <- ames[-index_1, ]  

nrow(train_1)/nrow(ames)
```

]

.pull-right[

Put the selected records in training set `train_1` by subsetting the original data frame `ames` with the row numbers stored in `index_1`.

]

---

# Data splitting in base

We first demonstrate the splitting of the `ames` housing data into a training and test set, using `base` R instructions. 

.pull-left[

```{r eval = FALSE}
set.seed(123) 
index_1 <- sample(1 : nrow(ames), 
                  size = round(nrow(ames) * 0.7))   
train_1 <- ames[index_1, ]   
test_1  <- ames[-index_1, ]  #<<

nrow(train_1)/nrow(ames)
```

]

.pull-right[

Put the not selected records in test set `test_1`.

]

---

# Data splitting in base

We first demonstrate the splitting of the `ames` housing data into a training and test set, using `base` R instructions. 

.pull-left[

```{r eval = TRUE}
set.seed(123) 
index_1 <- sample(1 : nrow(ames), 
                  size = round(nrow(ames) * 0.7))   
train_1 <- ames[index_1, ]   
test_1  <- ames[-index_1, ]  

nrow(train_1)/nrow(ames)   #<<
```

]

.pull-right[

What is the ratio of the number of records in `train_1` versus original data set `ames`?

]

---

# Data splitting in {caret}

The {caret} package - short for Classification And REgression Training - contains functions to streamline the model training process for complex regression and classification problems.

With the {caret} package, the function `createDataPartition` will do the job. 

.pull-left[

```{r eval = FALSE}
library(caret) #<<
set.seed(123)  #<<
index_2 <- caret::createDataPartition(
                    y = ames$Sale_Price, 
                    p = 0.7, 
                    list = FALSE)
train_2 <- ames[index_2, ]
test_2  <- ames[-index_2, ]

nrow(train_2)/nrow(ames) 
```

]

.pull-right[

Load the library {caret}.

Use `set.seed()` for reproducibility.

]

---

# Data splitting in {caret}

The {caret} package - short for Classification And REgression Training - contains functions to streamline the model training process for complex regression and classification problems.

With the {caret} package, the function `createDataPartition` will do the job. 

.pull-left[

```{r eval = FALSE}
library(caret) 
set.seed(123)  
index_2 <- caret::createDataPartition(    #<<
                    y = ames$Sale_Price,  #<<
                    p = 0.7,          #<<
                    list = FALSE)     #<<
train_2 <- ames[index_2, ]
test_2  <- ames[-index_2, ]

nrow(train_2)/nrow(ames) 
```

]

.pull-right[

`createDataPartition` takes in `y` the vector of outcomes of the data set we wish to split. `createDataPartition` will do stratified sampling based on levels of `y` (for factor) or groups determined by the percentiles of `y` (for numeric).

The percentage of data that goes to training is `p`.

`list = FALSE` tells the function not to store the results in a list, but in a matrix (here: with 1 column)

]



---

# Data splitting in {rsample}  <img src="img/rsample.png" class="title-hex">

The {rsample} package, part of the {tidymodels} initiative of RStudio, is home to a wide very variety of resampling functions. 

The documentation is at [rsample: the basics](https://tidymodels.github.io/rsample/articles/Basics.html).

.pull-left[
```{r eval = FALSE}
library(rsample) #<<
set.seed(123)  #<<
split_1  <- rsample::initial_split(ames, prop = 0.7)
train_3  <- training(split_1)
test_3   <- testing(split_1)

nrow(train_3)/nrow(ames)
```
]

.pull-right[

Load the `rsample` package.

Use `set.seed()` for reproducibility.

]

---

# Data splitting in {rsample}  <img src="img/rsample.png" class="title-hex">

The {rsample} package, part of the {tidymodels} initiative of RStudio, is home to a wide very variety of resampling functions. 

The documentation is at [rsample: the basics](https://tidymodels.github.io/rsample/articles/Basics.html).

.pull-left[
```{r eval = FALSE}
library(rsample)
set.seed(123) 
split_1  <- rsample::initial_split(ames, prop = 0.7)  #<<
train_3  <- training(split_1)
test_3   <- testing(split_1)

nrow(train_3)/nrow(ames)
```
]

.pull-right[

`initial_split` from the {rsample} package. 

Split the data `ames` into a training set and testing set.

`prop` is the proportion of data to be retained as training

]

---

# Data splitting in {rsample} <img src="img/rsample.png" class="title-hex">

The {rsample} package, part of the {tidymodels} initiative of RStudio, is home to a wide very variety of resampling functions. 

The documentation is at [rsample: the basics](https://tidymodels.github.io/rsample/articles/Basics.html).

.pull-left[
```{r eval = FALSE}
library(rsample)
set.seed(123) 
split_1  <- rsample::initial_split(ames, prop = 0.7)  
train_3  <- training(split_1) #<<
test_3   <- testing(split_1)  #<<

nrow(train_3)/nrow(ames)
```
]

.pull-right[

The result of `rsample::initial_split` is an `rset` object.

It is stored in `split_1` and ready for inspection.

Apply the functions `training` and `test` to this object to extract the data in each split.

]

---

# Data splitting comparison

As a check, we plot the `Sale_Price` as available in the train (in black) vs test (in red) data sets, created by each of the three demonstrated methods.

```{r plot-train-test, echo=FALSE}
ames <- AmesHousing::make_ames()

set.seed(123) 
index_1 <- sample(1 : nrow(ames), 
                  size = round(nrow(ames) * 0.7))   
train_1 <- ames[index_1, ]   
test_1  <- ames[-index_1, ] 

library(caret) 
set.seed(123)  
index_2 <- caret::createDataPartition(
                    y = ames$Sale_Price,  
                    p = 0.7,          
                    list = FALSE)     
train_2 <- ames[index_2, ]
test_2  <- ames[-index_2, ]

library(rsample)
set.seed(123) 
split_1  <- rsample::initial_split(ames, prop = 0.7)  
train_3  <- training(split_1) 
test_3   <- testing(split_1) 

p_1 <- ggplot(train_1, aes(x = Sale_Price)) + theme_bw() +
       geom_density(trim = TRUE) +
       geom_density(data = test_1, trim = TRUE, col = "red") +
       ggtitle("base R")

p_2 <- ggplot(train_2, aes(x = Sale_Price)) + theme_bw() +
    geom_density(trim = TRUE) +
    geom_density(data = test_2, trim = TRUE, col = "red") +
    theme(axis.title.y = element_blank(),
          axis.ticks.y = element_blank(),
          axis.text.y = element_blank()) +
    ggtitle("caret") 

p_3 <- ggplot(train_3, aes(x = Sale_Price)) + theme_bw() +
    geom_density(trim = TRUE) + 
    geom_density(data = test_3, trim = TRUE, col = "red") +
    theme(axis.title.y = element_blank(),
          axis.ticks.y = element_blank(),
          axis.text.y = element_blank()) +
    ggtitle("rsample")
```

```{r echo = FALSE, fig.align = 'center', fig.width = 10, fig.height = 4.5, dev = "svg"}
# side-by-side plots
gridExtra::grid.arrange(p_1, p_2, p_3, nrow = 1)

# clean up
rm(p_1, p_2, p_3)
```


---


# Resampling methods

In [Data splitting](#data-splitting), we discussed *training* and *test* set. Let's now dive deeper into *resampling* methods.

.hi-KUL[Validation set] (visual inspired by [Ed Rubin's course](https://github.com/edrubin/EC524W20))

- we hold out a subset of the training data (e.g. 30%) and then evaluate the model on this held out validation set

- calculate the loss function on this validation set, as approximation of the true test error

- .font140[`r fa(name = "far fa-thumbs-down", fill = KULbg)`] high variability + inefficient use of data

- picture .hi-KUL[validation set (30%)] and .hi-pink[training set (70%)]

```{r, data-validation-set, include = F, cache = T}
library(tidyverse)
# generate data
X = 40
Y = 12
set.seed(12345)
v_df = expand_grid(
  x = 1:X,
  y = 1:Y
) %>% mutate(grp = sample(
  x = c("Train", "Validate"),
  size = X * Y,
  replace = T,
  prob = c(0.7, 0.3)
)) %>% mutate(
  grp_2 = c(
    rep("Validate", sum(grp == "Validate")),
    rep("Train", sum(grp == "Train"))
  )
)
```

```{r, plot-validation-set, echo = F, dependson = "data-validation-set", fig.height = 2.3, cache = T}
ggplot(data = v_df, aes(x, y, fill = grp_2, color = grp_2)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual(" ", values = c("white", KULbg)) +
scale_color_manual(" ", values = c(red_pink , KULbg)) +
theme_void() +
theme(legend.position = "none")
```

---

# Resampling methods (cont.)

In [Data splitting](#data-splitting), we discussed *training* and *test* set. Let's now dive deeper into *resampling* methods.

.hi-KUL[*k* fold cross validation] (visual inspired by [Ed Rubin's course](https://github.com/edrubin/EC524W20))

- divide training data into *k* equally sized groups (e.g. .hi-KUL[group 1] on the picture)

- iterate over the *k* groups, treating each as validation set once (and train model on the other *k-1* groups) (e.g. get .hi-KUL[MSE<sub>1<sub/>] corresponding to fold 1)

- average the folds' loss to estimate the true test error

- .font140[`r fa(name = "far fa-thumbs-up", fill = KULbg)`] greater accuracy (compared to validation set).

```{r, data-CV-set, include = F, cache = T}
library(tidyverse)
# Generate data
X = 40
Y = 12
set.seed(12345)
cv_df = expand_grid(
  x = 1:X,
  y = 1:Y
) %>% mutate(
  id = 1:(X*Y),
  grp = sample(X * Y) %% 5 + 1
)
# Find groups
a = seq(1, X*Y, by = X*Y/5)
b = c(a[-1] - 1, X*Y)
```

```{r, plot-CV-set, echo = F, dependson = "data-CV-set", fig.height = 2.3, cache = T}
ggplot(
  data = cv_df,
  aes(x, y, color = between(id, a[1], b[1]), fill = between(id, a[1], b[1]))
) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", KULbg)) +
scale_color_manual("", values = c(red_pink, KULbg)) +
theme_void() +
theme(legend.position = "none")
```

---

# Resampling methods (cont.)

In [Data splitting](#data-splitting), we discussed *training* and *test* set. Let's now dive deeper into *resampling* methods.

.hi-KUL[*k* fold cross validation] (visual inspired by [Ed Rubin's course](https://github.com/edrubin/EC524W20))

- divide training data into *k* equally sized groups (e.g. .hi-KUL[group 1] on the picture)

- iterate over the *k* groups, treating each as validation set once (and train model on the other *k-1* groups) (e.g. get .hi-KUL[MSE<sub>1<sub/>] corresponding to fold 1)

- average the folds' loss to estimate the true test error

- .font140[`r fa(name = "far fa-thumbs-up", fill = KULbg)`] greater accuracy (compared to validation set).

```{r, plot-CV3-set, echo = F, dependson = "data-CV-set", fig.height = 2.3, cache = T}
ggplot(
  data = cv_df,
  aes(x, y, color = between(id, a[3], b[3]), fill = between(id, a[3], b[3]))
) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", KULbg)) +
scale_color_manual("", values = c(red_pink, KULbg)) +
theme_void() +
theme(legend.position = "none")
```

---

# Resampling methods (cont.)

In [Data splitting](#data-splitting), we discussed *training* and *test* set. Let's now dive deeper into *resampling* methods.

.hi-KUL[*k* fold cross validation] (picture from [Boehmke & Greenwell](https://koalaverse.github.io/homlr/))

.center[
```{r out.width = '95%', echo=FALSE}
knitr::include_graphics("img/k_fold_CV.png")
```
]

---

# Resampling methods (cont.)

In [Data splitting](#data-splitting), we discussed *training* and *test* set. Let's now dive deeper into *resampling* methods.

.hi-KUL[Leave-one-out cross validation] (visual inspired by [Ed Rubin's course](https://github.com/edrubin/EC524W20))

- each observation takes a turn as the validation set (e.g. get .hi-KUL[MSE<sub>3<sub/>])

- other *n-1* observations are the training set

- average the folds' loss to estimate the true test error

- .font140[`r fa(name = "far fa-thumbs-down", fill = KULbg)`] very computationally demanding. 

```{r, data-loocv, include = F, cache = T}
# Generate data
X = 40
Y = 12
loocv_df = expand_grid(
  x = 1:X,
  y = -(1:Y)
) %>% mutate(
  i = 1:(X * Y),
  grp_1 = if_else(i == 1, "Validate", "Train"),
  grp_2 = if_else(i == 2, "Validate", "Train"),
  grp_3 = if_else(i == 3, "Validate", "Train"),
  grp_4 = if_else(i == 4, "Validate", "Train"),
  grp_5 = if_else(i == 5, "Validate", "Train"),
  grp_n = if_else(i == X*Y, "Validate", "Train")
)
```


```{r, plot-LOOCV-set, echo = F, dependson = "data-loocv", fig.height = 2.3, cache = T}
ggplot(data = loocv_df, aes(x, y, fill = grp_3, color = grp_3)) +
geom_point(shape = 21, size = 4.5, stroke = 0.5) +
scale_fill_manual("", values = c("white", KULbg)) +
scale_color_manual("", values = c(red_pink, KULbg)) +
theme_void() +
theme(legend.position = "none")
```

---

# Resampling methods in {caret}

We set up 5-fold cross validation using the {caret} package.

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_folds <- caret::createFolds(y = ames$Sale_Price,        #<<
                               k = 5, list = TRUE,     #<<
                               returnTrain = TRUE)     #<<
```

```{r eval = FALSE}
str(cv_folds)
```

```{r echo = FALSE}
set.seed(123)  
cv_folds <- caret::createFolds(ames$Sale_Price, 
                        k = 5, list = TRUE, 
                        returnTrain = TRUE)
str(cv_folds)
```
]

.pull-right[

The `createFolds` function from {caret} splits the data into `k` groups.

`list = TRUE` indicates that the results should be stored in a list

`returnTrain = TRUE` indicates that the values returned (and stored) in the elements of the list are - per fold - the row numbers of the observations selected for training.

]

---

# Resampling methods in {caret}

We set up 5-fold cross validation using the {caret} package.

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_folds <- caret::createFolds(y = ames$Sale_Price,        
                               k = 5, list = TRUE,     
                               returnTrain = TRUE)    
```

```{r eval = FALSE}
str(cv_folds) #<<
```

```{r echo = FALSE}
set.seed(123)  
cv_folds <- caret::createFolds(ames$Sale_Price, 
                        k = 5, list = TRUE, 
                        returnTrain = TRUE)
str(cv_folds)
```
]

.pull-right[

Inspect the list `cv_folds` that was returned by `createFolds(.)`. 

This list has `k` elements, each storing the row numbers of the observations in the training set of the fold under consideration.

]

---

# Resampling methods in {caret} <img src="img/purrr.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
mean(ames[cv_folds$Fold1, ]$Sale_Price) #<<
```

```{r echo = FALSE}
mean(ames[cv_folds$Fold1, ]$Sale_Price)
```

```{r eval = FALSE}
map_dbl(cv_folds,                     
        function(x) {                   
          mean(ames[x, ]$Sale_Price)    
        })                              
```

```{r echo = FALSE}
map_dbl(cv_folds,
        function(x) {
          mean(ames[x, ]$Sale_Price)
        })
```

]

.pull-right[

We calculate the average `Sale_Price` per fold, that is: we average the `Sale_Price` over all observations selected in the training set of a particular fold. 

That would go as follows, for `Fold1` in the list `cv_folds`

```{r eval=FALSE}
mean(ames[cv_folds$Fold1, ]$Sale_Price)
```

and similarly for `Fold2`, ..., `Fold5`. 

]

---

# Resampling methods in {caret} <img src="img/purrr.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
mean(ames[cv_folds$Fold1, ]$Sale_Price) 
```

```{r echo = FALSE}
mean(ames[cv_folds$Fold1, ]$Sale_Price)
```

```{r eval = FALSE}
map_dbl(cv_folds,                     #<<
        function(x) {                 #<<  
          mean(ames[x, ]$Sale_Price)  #<<  
        })                            #<<  
```

```{r echo = FALSE}
map_dbl(cv_folds,
        function(x) {
          mean(ames[x, ]$Sale_Price)
        })
```



]

.pull-right[

We apply the function `mean(ames[___, ]$Sale_Price)` over all `k` elements of the list `cv_folds`.

`map_dbl(.x, .f)` is one of the `map` functions from the {purrr} package (part of {tidyverse}), used for functional programming in R.

`map_dbl(.x, .f)` applies function `.f` to each element of list `.x`.

The result is a double-precision vector, hence `map_dbl` and not just `map`.

Btw, it is a historical anomaly that R has two names for its floating-point vectors, `double` and `numeric`.

]

---

# Resampling methods in {rsample} <img src="img/rsample.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5) #<<
cv_rsample$splits
```

```{r echo = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5)
cv_rsample$splits
```

]

.pull-right[

The function `vfold_cv` splits the data into `v` groups (called folds) of equal size. 

]

---

# Resampling methods in {rsample} <img src="img/rsample.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5) 
cv_rsample$splits  #<<
```

```{r echo = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5)
cv_rsample$splits
```

]

.pull-right[

The function `vfold_cv` splits the data into `v` groups (called folds) of equal size. 

We store the result of `vfold_cv` in the object `cv_rsample`.

The resulting object stores `v` resamples of the original data set.

]

---

# Resampling methods in {rsample} <img src="img/rsample.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5) 
```

```{r echo = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5)
```

```{r eval = FALSE}
cv_rsample$splits[[1]] #<<
```

```{r echo = FALSE}
cv_rsample$splits[[1]]
```

```{r eval = FALSE}
cv_rsample$splits[[1]] %>% analysis() %>% dim()
```

```{r echo = FALSE}
cv_rsample$splits[[1]] %>% analysis() %>% dim()
```

```{r eval = FALSE}
cv_rsample$splits[[1]] %>% assessment() %>% dim()
```

```{r echo = FALSE}
cv_rsample$splits[[1]] %>% assessment() %>% dim()
```

]

.pull-right[

Inspect the composition of the first resample: 

2,344 (out of 2,930) observations go to the analysis data (for training, i.e. `v-1` folds), 

586 (out of 2,930) observations go to the assessment data (for testing, the final fold).   

]

---

# Resampling methods in {rsample} <img src="img/rsample.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5) 
```

```{r echo = FALSE}
set.seed(123)  
cv_rsample <- rsample::vfold_cv(ames, v = 5)
```

```{r eval = FALSE}
cv_rsample$splits[[1]] 
```

```{r echo = FALSE}
cv_rsample$splits[[1]]
```

```{r eval = FALSE}
cv_rsample$splits[[1]] %>% analysis() %>% dim() #<<
```

```{r echo = FALSE}
cv_rsample$splits[[1]] %>% analysis() %>% dim()
```

```{r eval = FALSE}
cv_rsample$splits[[1]] %>% assessment() %>% dim() #<<
```

```{r echo = FALSE}
cv_rsample$splits[[1]] %>% assessment() %>% dim()
```

]

.pull-right[

Inspect the composition of the first resample: 

get the dimensions (`dim()`) of the analysis data (`analysis()`) of the first resample 

get the dimensions (`dim()`) of the assessment data (`assessment()`) of the first resample. 

]

---

# Resampling methods in {rsample} <img src="img/purrr.png" class="title-hex"> <img src="img/rsample.png" class="title-hex">

.pull-left[

```{r eval = FALSE}
map_dbl(cv_rsample$splits,
        function(x) {
          mean(rsample::analysis(x)$Sale_Price)
        })
```

```{r echo = FALSE}
map_dbl(cv_rsample$splits,
        function(x) {
          mean(rsample::analysis(x)$Sale_Price)
        })
```

```{r eval = FALSE}
map_dbl(cv_rsample$splits,
        function(x) {
          nrow(rsample::analysis(x))
        })
```

```{r echo = FALSE}
map_dbl(cv_rsample$splits,
        function(x) {
          nrow(rsample::analysis(x))
        })
```

]

.pull-right[

As before, use `map_dbl(.x, .f)` to apply a function `.f` over all elements of a list `.x`. 

Here the list is stored in `cv_rsample$splits`, with `v = 5` elements. 

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

.hi-pink[Q]: Now you're going to combine data splitting and resampling to create training, validation and test folds in the Ames data.

Use `caret` or `rsample` and make the validation folds of the same size as the test fold.
]

---

class: clear

.pull-left[

with `caret`

```{r}
set.seed(5678)
ind_caret <- caret::createDataPartition(
                      y = ames$Sale_Price,
                      p = 5/6, list = FALSE)
train_caret <- ames[ind_caret, ]
test_caret  <- ames[-ind_caret, ]

cv_caret <- caret::createFolds(
            y = train_caret$Sale_Price, k = 5, 
            list = TRUE, returnTrain = FALSE)

purrr::map_dbl(cv_caret,
               ~ nrow(ames[., ]))
nrow(test_caret)
```


]

.pull-right[

with `rsample`

```{r}
set.seed(5678)
ind_rsample  <- rsample::initial_split(ames,
                                       prop = 5/6)
train_rsample  <- rsample::training(ind_rsample)
test_rsample   <- rsample::testing(ind_rsample)

cv_rsample <- rsample::vfold_cv(train_rsample, v = 5)

map_dbl(cv_rsample$splits,
        ~ nrow(rsample::assessment(.)))
nrow(test_rsample)
```

]

---

class: inverse, center, middle
name: tuning

# Parameter tuning with {caret}, {rsample} and {purrr}

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>


---

# Tuning parameters 

.pull-left[Finding the optimal level of flexibility highlights the bias-variance tradeoff.

.hi-pink[Bias] : the error that comes from inaccurately estimating $\color{#3b3b9a}{f}$.

.hi-pink[Variance] : the amount $\hat{\color{#3b3b9a}{f}}$ would change with a different training sample.

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] : high variance models more prone to overfitting

* use .hi-pink[resampling methods] to reduce this risk

* hyperparameters (or *tuning parameters*) control complexity, 
and thus the bias-variance trade-off

* identify their optimal setting, e.g. with a *grid search*

* no analytic expression for these hyperparameters.
]

.pull-right[

```{r bias-variance-knn, echo = FALSE, fig.width = 7, fig.height = 3, out.width = "100%"}
library(caret)
# Simulate some nonlinear monotonic data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 4.5)

KULbg <- "#116E8A"

# Single biased model fit
bias_model <- lm(y ~ I(x^3), data = df)
df$predictions <- predict(bias_model, df)
p_1 <- ggplot(df, aes(x, y)) +
  geom_point(alpha = .3) +
  geom_line(aes(x, predictions), size = 1.5, color = KULbg) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) + theme_bw() +
  ggtitle("Biased model fit")

p_1
# Single high variance model fit
variance_model <- knnreg(y ~ x, k = 3, data = df)
df$predictions <- predict(variance_model, df)
p_2 <- ggplot(df, aes(x, y)) +
  geom_point(alpha = .3) +
  geom_line(aes(x, predictions), size = 1.5, color = KULbg) +
  scale_y_continuous("Response", limits = c(-1.75, 1.75), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 4.5), expand = c(0, 0)) + theme_bw() +
  ggtitle("High variance model fit")

p_2
```

.footnote[Code from Boehmke & Greenwell (2019, Chapter 2) on [Hands-on machine learning with R](https://koalaverse.github.io/homlr/).]

]



---

# Tuning parameters via grid search

.pull-left-alt[

.center[
```{r out.width = '85%', echo = FALSE}
knitr::include_graphics("img/flow_chart_applied_predictive_modeling.jpg")
```
]
]

.pull-right-alt[

.hi-pink[Model training & validation phase]

- define a set of candidate values (a *grid*)

- assess model utility across the candidates (use clever *resampling*)

- choose the optimal settings (optimize *loss*)

- refit the model on entire training data with final tuning parameters

- evaluate performance of the model on the test data (under `r fa('fas fa-lock', fill = KULbg)`). 

.hi-pink[Model selection] 

- repeat the above steps for different models 

- compare performance of these models that will generalize to new data (via test data, under `r fa('fas fa-lock', fill = KULbg)`).

.footnote[Flow chart from Kuhn & Johnson (2013) on [Applied predictive modeling](http://appliedpredictivemodeling.com/).]
]

---

# Training a model with {caret}

.pull-left[

```{r eval=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, #<<
                   returnResamp = "all",  
                   selectionFunction = "best")
hyper_grid <- expand.grid(k = seq(2, 150, by = 2)) 
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid)
knn_fit$bestTune
```

]

.pull-right[

Use `trainControl` from {caret} to set some control parameters that will be used in the actual `train` function. 

Here, we use `method = cv` and `number = 5` for 5-fold cross validation. 

]

---

# Training a model with {caret}

.pull-left[

```{r eval=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  #<<
                   selectionFunction = "best") #<<
hyper_grid <- expand.grid(k = seq(2, 150, by = 2)) 
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid)
knn_fit$bestTune
```

]

.pull-right[

In `trainControl` we put `returnResamp = "all"` to store all resampled summary metrics.

`selectionFunction = "best"` specifies how we select the optimal tuning parameter. With `"best"` the value that minimizes the performance (here: RMSE) is selected. 

Alternative: `selectionFunction = "oneSE"` applies the one standard error rule. 

]

---

# Training a model with {caret}

.pull-left[

```{r eval=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "best") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  #<<
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid)
knn_fit$bestTune
```

]

.pull-right[

Set the grid of *K*-values that will be searched. 

`expand.grid` creates a data frame with one row for each value of *K* to consider. 

]

---

# Training a model with {caret}

.pull-left[

```{r eval=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "best") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
knn_fit <- train(y ~ x, data = df, method = "knn", #<<
                        trControl = cv, #<<
                        tuneGrid = hyper_grid) #<<
knn_fit$bestTune
```

]

.pull-right[

{caret} will `train` the method `knn` using the settings in `trControl = cv`, across the values of *K* stored in `tuneGrid = hyper_grid`.

The data `df` and formula `y ~ x` are used.

]

---

# Training a model with {caret}

.pull-left[

```{r eval=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "best") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid) 
knn_fit$bestTune  #<<
```

```{r echo=FALSE}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "best") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid) 
knn_fit$bestTune  #<<
```

]

.pull-right[

We retrieve the optimal value of the tuning parameter, according to the `selectionFunction`. 

For the folds created here and with `selectionFunction = "best"` the optimal *K* value is 36. 

What happens when you change to `selectionFunction = "oneSE"`?

]

---

# Training a model with {caret}

.pull-left[

```{r echo=FALSE, fig.width = 10, fig.height = 6.0}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "best") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid) 
knn_fit$bestTune  

ggplot() + theme_bw() +
  geom_line(data = knn_fit$results, aes(k, RMSE)) +
  geom_point(data = knn_fit$results, aes(k, RMSE)) +
  geom_point(data = filter(knn_fit$results, k == as.numeric(knn_fit$bestTune)),
             aes(k, RMSE),
             shape = 21,
             fill = "yellow",
             color = "black",
             stroke = 1,
             size = 4) +
  scale_y_continuous("Error (RMSE)")
```

]

.pull-right[

```{r echo=FALSE, fig.width = 10, fig.height = 6.0}
set.seed(123)
cv <- trainControl(method = "cv", number = 5, 
                   returnResamp = "all",  
                   selectionFunction = "oneSE") 
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
knn_fit <- train(y ~ x, data = df, method = "knn", 
                        trControl = cv, 
                        tuneGrid = hyper_grid) 
knn_fit$bestTune  

ggplot() + theme_bw() +
  geom_line(data = knn_fit$results, aes(k, RMSE)) +
  geom_point(data = knn_fit$results, aes(k, RMSE)) +
  geom_point(data = filter(knn_fit$results, k == as.numeric(knn_fit$bestTune)),
             aes(k, RMSE),
             shape = 21,
             fill = "yellow",
             color = "black",
             stroke = 1,
             size = 4) +
  scale_y_continuous("Error (RMSE)")
```

]

---

# Training a model with {rsample} <img src="img/rsample.png" class="title-hex"> <img src="img/purrr.png" class="title-hex">

.pull-left[

Our starting point is the simulated data stored in `df`, resampled with 5-fold cross-validation.

```{r eval=FALSE}
set.seed(123)  # for reproducibility
cv_rsample <- vfold_cv(df, 5)
cv_rsample$splits[1:3] 
```

```{r echo=FALSE}
set.seed(123)  # for reproducibility
cv_rsample <- vfold_cv(df, 5)
cv_rsample$splits[1:3] 
```
]

.pull-right[

We fit the *KNN* on the holdout data in split *s*, using a given *K* value. 

```{r eval=FALSE}
holdout_results <- function(s, k_val) {
  # Fit the model to the analysis data in split s
  df_train <- analysis(s)
  mod <- knnreg(y ~ x, k = k_val, data = df_train)
  # Get the remaining group
  holdout <- assessment(s)
  # Get predictions with the holdout data set
  res <- predict(mod, newdata = holdout)
  # Return observed and predicted values 
  #                            on holdout set
  res <- tibble(obs = holdout$y, pred = res)
  res
}
```

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Now you're going to combine the .hi-pink[resampling and model fitting instructions] and set up a first example of .hi-pink[tuning a parameter] over a grid of possible values: the *K* in a .hi-pink[KNN regression model]. 

.hi-pink[Q]: use the function `holdout_results(.s, .k)` as defined on the previous sheet. You will use this function to calculate the RMSE<sub>k</sub> of fold *k*. 

1. Specify a grid of values of *K*, store it in `hyper_grid`. Use `expand.grid(.)`

2. Pick one of the resamples stored in `cv_rsample$splits` and pick a value from the grid. Calculate the RMSE on the holdout data of this split. 

3. For all values in the tuning grid, calculate the RMSE averaged over all folds, and the corresponding standard error. 

4. Use the results from .hi-pink[Q.3] to pick the value of *K* via minimal RMSE.

5. Pick the largest value of *K* such that the corresponding RMSE is below the minimal RMSE from .hi-pink[Q.4] plus its corresponding SE. 

]

---

class: clear

.pull-left[

.hi-pink[Q.1] We set up the grid

```{r eval=FALSE}
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
hyper_grid %>% slice(1:3) 
```

```{r echo=FALSE}
hyper_grid <- expand.grid(k = seq(2, 150, by = 2))  
hyper_grid %>% slice(1:3) %>% kable()
```

.hi-pink[Q.2] We apply the function `holdout_results(.s, .k)` on the 
third resample, with the first value for *K* in the grid.

```{r eval=FALSE}
res <- holdout_results(cv_rsample$splits[[3]], 
                       hyper_grid[1, ])
sqrt(sum((res$obs - res$pred)^2)/nrow(res))
```

```{r echo=FALSE}
holdout_results <- function(s, k_val) {
  # Fit the model to the analysis data in split s
  df_train <- analysis(s)
  mod <- knnreg(y ~ x, k = k_val, data = df_train)
  # Get the remaining group
  holdout <- assessment(s)
  # Get predictions with the holdout data set
  res <- predict(mod, newdata = holdout)
  # Return observed and predicted values 
  #                            on holdout set
  res <- tibble(obs = holdout$y, pred = res)
  res
}
res <- holdout_results(cv_rsample$splits[[3]], hyper_grid[1, ])
sqrt(sum((res$obs - res$pred)^2)/nrow(res))
```
]

.pull-right[

.hi-pink[Q.3] Mean RMSE over the 5 folds and corresponding SE.

```{r eval=FALSE}
RMSE <- numeric(nrow(hyper_grid))
SE <- numeric(nrow(hyper_grid))
for(i in 1:nrow(hyper_grid)){
  cv_rsample$results <- map(cv_rsample$splits,
                            holdout_results,
                            hyper_grid[i, ])
  res <- map_dbl(cv_rsample$results, 
                 function(x) mean((x$obs - x$pred)^2))
  RMSE[i] <- mean(sqrt(res)) ; SE[i] <- sd(sqrt(res))
}


```

.hi-pink[Q.4] Choose *K* via minimal RMSE 

```{r echo=FALSE}
RMSE <- numeric(nrow(hyper_grid))
SE <- numeric(nrow(hyper_grid))
for(i in 1:nrow(hyper_grid)){
  cv_rsample$results <- map(cv_rsample$splits,
                      holdout_results,
                      hyper_grid[i, ])
  res <- map_dbl(cv_rsample$results, 
                 function(x) mean((x$obs - x$pred)^2))
  RMSE[i] <- mean(sqrt(res)) ; SE[i] <- sd(sqrt(res))
}

df <- tibble(RMSE, SE, k = hyper_grid$k)
df <- df %>% mutate(lower = RMSE-SE, upper = RMSE + SE)
best <- df %>% filter(RMSE == min(RMSE)) 
best %>% kable(format = 'html')
```

.hi-pink[Q.5] Choose *K* via the one-standard-error rule
```{r echo=FALSE}
one_SE <- df %>% filter(RMSE <= best$upper) %>% filter(k == max(k)) 
one_SE %>% kable(format = 'html')
```

]

---

class: clear


```{r one-SE-rule, echo=FALSE, fig.width = 10, fig.height = 6}
big_g <- ggplot(data = df) + theme_bw() +
  geom_line(aes(k, RMSE)) +
  geom_point(aes(k, RMSE)) +
  geom_pointrange(aes(k, RMSE, ymin = lower, ymax = upper), alpha = 0.5) +
  geom_point(data = best,
             aes(k, RMSE),
             shape = 21,
             fill = "yellow",
             color = "black",
             stroke = 1,
             size = 2) +
  geom_point(data = one_SE,
             aes(k, RMSE),
             shape = 21,
             fill = "yellow",
             color = "black",
             stroke = 1,
             size = 2) +
  geom_hline(data = best,
            aes(yintercept = upper)) +
  scale_y_continuous("Error (RMSE)")
big_g
```

---

# Putting it all together

.pull-left[

During the tuning process we inspect plots like the one on the right. 

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] &nbsp; &nbsp; *Less is more*: 

- we prefer simple over more complex 

- choose tuning parameters based on the numerically optimal value .KULbginline[OR]

- choose a simpler model that is within a certain tolerance of the
numerically best value 

- use the .hi-pink['one-standard-error' rule].

With the selected tuning parameters, we refit the model on the complete training set and use it to predict the test set (under `r fa('fas fa-lock', fill = KULbg)`). 

]

.pull-right[

```{r echo=FALSE, dependson='one-SE-rule', out.width='85%'}
big_g
```

]


---

class: inverse, center, middle
name: engineering

# Target and feature engineering: <br> data pre-processing steps


<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# What is feature engineering?

.pull-left-alt[

.center[
<img src="img/feature_engineering_Kuhn.jpg" alt="Drawing" style="width: 165px;"/>
<br> <br> <img src="img/boehmke_greenwell.jpg" alt="Drawing" style="width: 165px;"/>  
]

]

.pull-right-alt[
Feature engineering:

- applies .hi-pink[pre-processing steps] to predictor (features) variables

- .hi-pink[creates new input features] from your existing ones (e.g. network features derived from a social network in a fraud detection model).

Target engineering: 

* transforms the response variable (or target) to improve the performance of a predictive model.

The goal is to .KULbginline[make models more effective].

See Kuhn & Johnson (2019) on [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/) for a detailed discussion. 

]

---

name: black-white-box
class: clear

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] : *different models* have *different sensitivities* to the type of target and feature values in the model.

.center[
```{r out.width = '65%', echo=FALSE}
knitr::include_graphics("img/Applied_Pred_overview_models.jpg")
```
]

Source: Kuhn & Johnson (2013) on [Applied predictive modeling](http://appliedpredictivemodeling.com/).

---

# Target engineering <img src="img/rsample.png" class="title-hex"> 

We load the `ames` data set from the {AmesHousing} package and apply a .hi-pink[stratified split] of the data into a training (70%) and test (30%) set.

We stratify on the distribution of the target variable `Sale_Price` using the `strata` argument in `rsample::initial_split`.

```{r eval=FALSE}
ames <- AmesHousing::make_ames()
set.seed(123)  
split  <- rsample::initial_split(ames, prop = 0.7, 
                                       `strata = "Sale_Price"`) 
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

```{r echo=FALSE}
ames <- AmesHousing::make_ames()
set.seed(123)  
split  <- rsample::initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

We check the distribution of `Sale_Price` in both `ames_train` and `ames_test`.

```{r eval=FALSE}
summary(ames_train$Sale_Price)
summary(ames_test$Sale_Price)
```

```{r echo=FALSE}
summary(ames_train$Sale_Price)
summary(ames_test$Sale_Price)
```
---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Inference with linear models often assumes that the target is generated from a normal distribution.

.hi-pink[Q]: let's examine whether the `Sale_Price` target satisfies this assumption.

1. Plot a histogram of `Sale_Price`. Is normality a meaningful assumption?

2. Try some transformation functions such that the transformed target approaches a normal distribution.  
]

---

class: clear

.pull-left[
.hi-pink[Q.1] original target

```{r, echo = FALSE, out.width='80%'}
ggplot(data = ames_train, aes(x = Sale_Price)) + 
  geom_histogram(bins = 50, fill = KULbg, col = KULbg, alpha = 0.5) +
  theme_bw() + ggtitle("AMES - original target")
```

```{r}
summary(ames_train$Sale_Price)
```

]

.pull-right[

.hi-pink[Q.2] log-transformed target

```{r, echo = FALSE, out.width='80%'}
ggplot(data = ames_train, aes(x = log(Sale_Price))) + 
  geom_histogram(bins = 50, fill = KULbg, col = KULbg, alpha = 0.5) +
  theme_bw() + ggtitle("AMES - tranformed target")
```

```{r}
summary(log(ames_train$Sale_Price))
```
]

---

# Feature engineering steps

Examples of common pre-processing steps:

* Some models (e.g. KNN, Lasso, neural networks) require that the predictor variables are on the same scale. 
<br>
.hi-pink[Centering (C)] and .hi-pink[scaling (S)] the predictors can be used for this purpose.

* Other models are very sensitive to correlations between the predictors and filters or PCA signal extraction can improve the model.

* Some models find .hi-pink[(near) zero-variance (NZV)] predictors problematic, and these should be removed before fitting the model. 

* In other cases, the data should be .hi-pink[encoded] in a specific way to make sure all predictors are numeric (e.g. one-hot encoding of factor variables in neural networks). 

* Many models cannot cope with .hi-pink[missing data] so .hi-pink[imputation strategies] might be necessary.

* Development of new features that represent something important to the outcome. 

* (add your own example here!)

This list is inspired by Max Kuhn (2019) on [Applied Machine Learning](https://github.com/topepo/aml-london-2019). 

---

# A blueprint for feature engineering

.font140[.KULbginline[Take-aways]] .font160[`r fa(name = "fas fa-bullhorn", fill = KULbg)`] : *a proper implementation*

.pull-left[

* draft a .hi-pink[blueprint] of the necessary pre-processing steps, and their order 

* [Boehme & Greenwell (2019)](https://bradleyboehmke.github.io/HOML/engineering.html#proper-implementation) suggest

&nbsp; &nbsp; &nbsp; &nbsp; 1. Filter out zero or near-zero variance features. <br>
&nbsp; &nbsp; &nbsp; &nbsp; 2. Perform imputation if required. <br>
&nbsp; &nbsp; &nbsp; &nbsp; 3. Normalize to resolve numeric feature skewness. <br>
&nbsp; &nbsp; &nbsp; &nbsp; 4. Standardize (center and scale) numeric features. <br>
&nbsp; &nbsp; &nbsp; &nbsp; 5. Perform dimension reduction (e.g., PCA) on <br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; numeric features. <br>
&nbsp; &nbsp; &nbsp; &nbsp; 6. One-hot or dummy encode categorical features.

]

.pull-right[

* avoid .hi-pink[data leakage] in the pre-processing steps when applied to resampled data sets! 

.center[
```{r out.width = '100%', echo=FALSE}
knitr::include_graphics("img/data_leakage.png")
```
]

]

---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

.pull-left[

We already detected the necessity of log-transforming `Sale_Price` when building linear models. 

We add another pre-processing step, inspired by the .hi-pink[high cardinality] feature `Neighborhood`. 

```{r eval=FALSE}
ames_train %>% group_by(Neighborhood) %>% 
  summarize(n_obs = n()) %>% 
  arrange(n_obs) %>% slice(1:4)
```

```{r echo=FALSE}
ames_train %>% group_by(Neighborhood) %>% summarize(n_obs = n()) %>% arrange(n_obs) %>% slice(1:4) %>% kable(format = 'html')
```

]

.pull-right[

```{r echo=FALSE, out.width='95%'}
df <- ames_train %>% group_by(Neighborhood) %>% summarize(n_obs = n()) %>% arrange(n_obs)

ggplot(ames_train, aes(x = fct_infreq(Neighborhood))) + theme_bw() +
  geom_bar(col = KULbg, fill = KULbg, alpha = .5) + 
  coord_flip() + 
  xlab("") 
```

]

---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

.pull-left[
We'll use `recipe()` from the {recipes} package. 

The main idea is to .hi-pink[preprocess multiple datasets] using a single `recipe()`.

Before we start, keep the following .KULbginline[fundamentals] of {recipes} in mind!
]

--

.pull-right[

Creating a `recipe` takes the following steps:

* get the *ingredients* (`recipe()`): specify the response and predictor variables

* write the recipe (`step_zzz()`): define the *pre-processing steps*, such as imputation, creating dummy variables, scaling, and more

* *prepare* the recipe (`prep()`): provide a dataset to base each step on (e.g. *calculate* constants to do centering and scaling)

* *bake* the recipe (`bake()`): *apply* the pre-processing steps to your datasets.

.footnote[Source: [Rebecca Barter's blog](http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/)]

]
---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

.pull-left[

Use `recipe()` to create the preprocessing blueprint (to be applied later)

```{r eval = FALSE}
library(recipes)
mod_rec <- recipe(Sale_Price ~ ., data = ames_train)
mod_rec
```

```{r echo = FALSE}
library(recipes)
mod_rec <- recipe(Sale_Price ~ ., data = ames_train)
mod_rec
```

Now, `mod_rec` knows the role of each variable (`predictor` or `outcome`). 

We can use selectors such as `all_predictors()`, `all_outcomes()` or `all_nominal()`.

]

.pull-right[

Extend `mod_rec` with two pre-processing steps: 

`step_log(all_outcomes())`

`step_other(Neighborhood, threshold = 0.05)` to lump the levels that occur in less than 5% of data as "other".

```{r eval=TRUE}
mod_rec <- mod_rec %>% step_log(all_outcomes()) %>%
           step_other(Neighborhood, threshold = 0.05)
mod_rec
```

]

---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

.center[
```{r out.width = '100%', echo=FALSE}
knitr::include_graphics("img/recipes_verbs.png")
```
]

.footnote[Source Max Kuhn (2019) on [Applied Machine Learning](https://github.com/topepo/aml-london-2019).]

Now that we have a preprocessing *specification*, we run on it on the `ames_train` to *prepare* (or `prep()`) the recipe.

```{r eval = FALSE}
mod_rec_trained <- prep(mod_rec, training = ames_train, verbose = TRUE, retain = TRUE)
```

```{r trained_recipe, echo = TRUE}
mod_rec_trained <- prep(mod_rec, training = ames_train, verbose = TRUE, retain = TRUE)
```

The `retain = TRUE` indicates that the preprocessed training set should be saved.
---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

```{r eval = FALSE}
mod_rec_trained
```

```{r echo = FALSE, dependson='trained_recipe'}
mod_rec_trained
```

Once the recipe is prepared, it can be applied to any data set using `bake()`. There is no need to `bake()` the data used in the `prep()` step; you get the processed training set with `juice()`.

```{r}
ames_test_prep <- bake(mod_rec_trained, new_data = ames_test)
```

---

# Feature engineering with {recipes} <img src="img/recipes.png" class="title-hex"> 

.pull-left[

```{r eval=FALSE}
ames_test_prep %>% group_by(Neighborhood) %>% 
  summarize(n_obs = n()) %>% 
  arrange(n_obs) 
```

```{r echo=FALSE}
ames_test_prep %>% group_by(Neighborhood) %>% 
  summarize(n_obs = n()) %>% 
  arrange(n_obs) %>% kable(format = 'html')
```
]

.pull-right[
```{r eval=FALSE}
juice(mod_rec_trained) %>% group_by(Neighborhood) %>% 
  summarize(n_obs = n()) %>% 
  arrange(n_obs) 
```

```{r echo=FALSE}
juice(mod_rec_trained) %>% group_by(Neighborhood) %>% 
  summarize(n_obs = n()) %>% 
  arrange(n_obs) %>% kable(format = 'html')
```
]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

Now you will extend the existing recipe in `mod_rec`, prepare and bake it again!

.hi-pink[Q]: consult the [{recipes} manual](https://tidymodels.github.io/recipes/reference/index.html) and specify a recipe for the housing data that includes the following pre-processing steps (in this order)

* log-transform the outcome variable
* remove any zero-variance predictors
* lump factor levels that occur in <= 5% of data as "other" for both `Neighborhood` as well as `House_Style`
* center and scale all numeric features.

1. Specify the above recipe on the training set and store it in the object `mod_rec`. 
2. Inspect the object `mod_rec` using `summary(mod_rec)`. What can you learn from this summary?
3. Prepare the recipe on the training data and then apply it to the test set. 

]

---

class: clear

.pull-left[

First, let's try to get a grasp of the `House_Style` feature as well as the presence of zero-variance predictors. 

```{r eval=FALSE}
ames_train %>% group_by(House_Style) %>% 
              summarize(n_obs = n()) %>% 
               arrange(n_obs)
```

```{r echo=FALSE}
ames_train %>% group_by(House_Style) %>% summarize(n_obs = n()) %>% arrange(n_obs) %>% kable(format = 'html')
```


]

.pull-right[

```{r echo=FALSE, out.width='100%'}
ggplot(ames_train, aes(x = fct_infreq(House_Style))) + theme_bw() +
  geom_bar(col = KULbg, fill = KULbg, alpha = .5) + 
  coord_flip() + 
  xlab("") 
```

]

---

class: clear

To detect the presence of zero-variance and near-zero-variance features the `caret` library has the function `nearZeroVar`

```{r eval=TRUE}
library(caret)
nzv <- caret::nearZeroVar(ames_train, saveMetrics = TRUE)
```

```{r eval=FALSE}
names(ames_train)[nzv$zeroVar]
```

```{r echo=FALSE}
names(ames_train)[nzv$zeroVar]
```

```{r eval=FALSE}
names(ames_train)[nzv$nzv]
```

```{r echo=FALSE}
names(ames_train)[nzv$nzv]
```

So, no features have zero- variance, but 20 features have near-zero-variance.

---

class: clear 

.pull-left[

We put the recipe together with the following steps

```{r eval=FALSE}
mod_rec <- recipe(Sale_Price ~ ., data = ames_train) %>%
        step_log(all_outcomes()) %>%
        step_other(Neighborhood, threshold = 0.05) %>%
        step_other(House_Style, threshold = 0.05) %>%
        step_zv(all_predictors()) %>% 
        step_nzv(all_predictors()) %>%
        step_center(all_numeric(), -all_outcomes()) %>%
        step_scale(all_numeric(), -all_outcomes())
summary(mod_rec) %>% slice(1:6)
```

```{r echo=FALSE}
mod_rec <- recipe(Sale_Price ~ ., data = ames_train) %>%
           step_log(all_outcomes()) %>%
           step_other(Neighborhood, threshold = 0.05) %>%
           step_other(House_Style, threshold = 0.05) %>%
           step_zv(all_predictors()) %>% 
           step_nzv(all_predictors()) %>%
           step_center(all_numeric(), -all_outcomes()) %>%
           step_scale(all_numeric(), -all_outcomes())
summary(mod_rec) %>% slice(1:6) %>% kable(format = 'html')
```

]

.pull-right[

```{r eval=FALSE}
mod_rec
```


```{r echo=FALSE}
mod_rec <- recipe(Sale_Price ~ ., data = ames_train) %>%
                  step_log(all_outcomes()) %>%
            step_other(Neighborhood, 
                         threshold = 0.05) %>%
            step_other(House_Style, 
                         threshold = 0.05) %>%
            step_zv(all_predictors()) %>% 
            step_nzv(all_predictors()) %>%
            step_center(all_numeric(), 
                          -all_outcomes()) %>%
            step_scale(all_numeric(), 
                          -all_outcomes())
mod_rec
```

]


---

class: clear

.pull-left[

We prep the recipe on `ames_train`

```{r eval = TRUE}
mod_rec_trained <- prep(mod_rec, 
                        training = ames_train, 
                        verbose = TRUE, retain = TRUE)
```

and bake it on the `ames_test` data

```{r}
ames_test_prep <- bake(mod_rec_trained, 
                                new_data = ames_test)
```

We inspect the processed training and test set

```{r eval=FALSE}
dim(juice(mod_rec_trained)) 
```

```{r echo=FALSE}
dim(juice(mod_rec_trained)) 
```

]

.pull-right[

Verify that `Sale_Price` is log-transformed (but not centred and scaled)

```{r eval=FALSE}
head(juice(mod_rec_trained)$Sale_Price) 
head(ames_train$Sale_Price)
head(ames_test_prep$Sale_Price)
head(ames_test$Sale_Price)
```

```{r echo=FALSE}
options(digits = 4)
head(juice(mod_rec_trained)$Sale_Price) 
```

```{r echo=FALSE}
options(digits = 4)
head(ames_train$Sale_Price) 
```

```{r echo=FALSE}
options(digits = 4)
head(ames_test_prep$Sale_Price) 
```

```{r echo=FALSE}
options(digits = 4)
head(ames_test$Sale_Price) 
```

```{r eval=FALSE}
levels(juice(mod_rec_trained)$House_Style)
```

```{r eval=FALSE}
levels(ames_test_prep$House_Style)
```

```{r echo=FALSE}
levels(juice(mod_rec_trained)$House_Style)[1:2]
levels(juice(mod_rec_trained)$House_Style)[3:4]
```

```{r echo=FALSE}
levels(ames_test_prep$House_Style)[1:2]
levels(ames_test_prep$House_Style)[3:4]
```

]

---

# Putting it all together {rsample} and {recipes} <img src="img/recipes.png" class="title-hex"> <img src="img/rsample.png" class="title-hex"> 

Let's redo the KNN example, with centering and scaling of the x-feature, by combining {rsample}/{caret} with a recipe. 

.pull-left[

```{r eval = TRUE}
# get the simulated data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>% filter(x < 4.5)
```

```{r eval = TRUE}
# specify the recipe
library(recipes)
rec <- recipe(y ~ x, data = df)
rec <- rec %>% step_center(all_predictors()) %>%
               step_scale(all_predictors())
```

]

.pull-right[

```{r eval = TRUE}
# doing this on complete data set df
rec_df <- prep(rec, training = df)
mean(juice(rec_df)$x) # centered!
sd(juice(rec_df)$x)   # scaled!
```

```{r}
# now we combine the recipe with rsample steps
library(rsample)
set.seed(123)  # for reproducibility
cv_rsample <- vfold_cv(df, 5)
```

```{r}
# we apply the steps in the recipe to each fold
library(purrr)
cv_rsample$recipes <- map(cv_rsample$splits, prepper, 
                          recipe = rec)
# check `?prepper`
```

]

---

# Putting it all together {rsample} and {recipes} <img src="img/recipes.png" class="title-hex"> <img src="img/rsample.png" class="title-hex"> 

Let's redo the KNN example, with centering and scaling of the x-feature, by combining {rsample}/{caret} with a recipe. 

.pull-left[

Now you can inspect `cv_rsample` as follows

```{r eval = FALSE}
cv_rsample$recipes[[1]]
juice(cv_rsample$recipes[[1]])
bake(cv_rsample$recipes[[1]], 
     new_data = assessment(cv_rsample$splits[[1]]))
```


]

.pull-right[

```{r}
holdout_results <- function(s, rec, k_val) {
  # Fit the model to the analysis data in split s
  df_train <- juice(rec)
  mod <- knnreg(y ~ x, k = k_val, data = df_train)
  # Get the remaining group
  holdout <- bake(rec, new_data = assessment(s))
  # Get predictions with the holdout data set
  res <- predict(mod, newdata = holdout)
  # Return observed and predicted values 
  #                            on holdout set
  res <- tibble(obs = holdout$y, pred = res)
  res
}
```

```{r}
res <- holdout_results(cv_rsample$splits[[2]], 
                       cv_rsample$recipes[[2]], 
                       k_val = 58)
sqrt(sum((res$obs - res$pred)^2)/nrow(res))
```

]

---

# Putting it all together {rsample} and {recipes} <img src="img/recipes.png" class="title-hex"> <img src="img/rsample.png" class="title-hex"> 

Let's redo the KNN example, with centering and scaling of the x-feature, by combining {rsample}/{caret} with a recipe. 

```{r}
RMSE <- numeric(nrow(hyper_grid))
SE <- numeric(nrow(hyper_grid))
for(i in 1:nrow(hyper_grid)){
  cv_rsample$results <- map2(cv_rsample$splits, cv_rsample$recipes,
                            holdout_results,
                            hyper_grid[i, ])
  res <- map_dbl(cv_rsample$results, 
                 function(x) mean((x$obs - x$pred)^2))
  RMSE[i] <- mean(sqrt(res)) ; SE[i] <- sd(sqrt(res))
}
```


---

class: inverse, center, middle
name: regression

# Regression models in R and <br> tidy model output with {broom}

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

name: models-in-R

# Creating models in R

The **formula** interface using R's [formula rules](https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Formulae-for-statistical-models) to specify a *symbolic* representation of the terms:

* response ~ variable, with `model_fn` referring to the specific model function you want to use, e.g. `lm` for linear regression

```{r eval=FALSE}
model_fn(Sale_Price ~ Gr_Liv_Area, data = ames)
```

* response ~ variable_1 + variable_2

```{r eval=FALSE}
model_fn(Sale_Price ~ Gr_Liv_Area + Neighborhood, data = ames)
```

* response ~ variable_1 + variable_2 + their interaction

```{r eval=FALSE}
model_fn(Sale_Price ~ Gr_Liv_Area + Neighborhood + Neighborhood:Gr_Liv_Area, data = ames)
```

* shorthand for all predictors

```{r eval=FALSE}
model_fn(Sale_Price ~ ., data = ames)
```



---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

You will now fit some linear regression models on the `ames` housing data. 
<br> <br> 
You will explore the model fits with `base` R instructions as well as the functionalities offered by the `broom` package.
<br> <br>
.hi-pink[Q]: load the `ames` housing data set via `ames <- AmesHousing::make_ames()`

1. Fit a linear regression model with `Sale_Price` as response and `Gr_Liv_Area` as covariate. Store the resulting object as `model_1`.

2. Repeat your instruction, but now put it between brackets. What happens?

3. Inspect `model_1` with the following set of instructions

- `summary(___)`
- extract the fitted coefficients, using `___$coefficients`
- what happens with `summary(___)$coefficients`?
- extract fitted values, using `___$fitted.values`
- now try to extract the R<sup>2</sup> of this model. 

]

---

class: clear

```{r, echo = FALSE}
ames <- AmesHousing::make_ames()
```

.hi-pink[Q.1] Linear model with `Sale_Price` as a function of `Gr_Live_Area`

```{r, eval = FALSE}
model_1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames)
```

```{r, echo = FALSE}
model_1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames)
```


.hi-pink[Q.3] Check `model_1` - What happens - do you *like* this display?

```{r, eval = FALSE}
summary(model_1)
```

Now let's extract some meaningful information from `model_1` (using `base` R instructions)

.pull-left[

```{r, eval = FALSE}
model_1$coefficients
```

```{r, echo = FALSE}
coef(model_1)
```

```{r, eval = FALSE}
summary(model_1)$coefficients
```

```{r, echo = FALSE}
summary(model_1)$coefficients
```

]

.pull-right[

```{r eval = FALSE}
head(model_1$fitted.values)
```

```{r echo = FALSE}
head(model_1$fitted.values)
```

```{r eval = FALSE}
summary(model_1)$r.squared
```

```{r echo = FALSE}
summary(model_1)$r.squared
```


]

---

# Tidy model output <img src="img/broom.png" class="title-hex">

The package {broom} allows to summarize key information about statistical objects (e.g. a linear regression model) in so-called tidy tibbles. 

This makes it easy to report results, create plots and consistently work with large numbers of models at once. 

We briefly illustrate the three essential verbs of `broom`: `tidy()`, `glance()` and `augment()`.

```{r eval = FALSE}
model_1 %>% broom::tidy() 
```

```{r echo = FALSE}
model_1 %>% broom::tidy() %>% kable(format = 'html')
```

```{r eval = FALSE}
model_1 %>% broom::glance() 
```

```{r echo = FALSE}
model_1 %>% broom::glance() %>% kable(format = 'html')
```

---

# Tidy model output <img src="img/broom.png" class="title-hex">

The package {broom} allows to summarize key information about statistical objects (e.g. a linear regression model) in so-called tidy tibbles. 

This makes it easy to report results, create plots and consistently work with large numbers of models at once. 

We briefly illustrate the three essential verbs of `broom`: `tidy()`, `glance()` and `augment()`.

```{r eval = FALSE}
model_1 %>% broom::augment() %>% slice(1:5)
```

```{r echo = FALSE}
model_1 %>% broom::augment() %>% slice(1:5) %>% kable(format = 'html')
```

---

class: clear

.pull-left[

```{r eval = FALSE}
g_lm_1 <- ggplot(data = ames, 
                 aes(Gr_Liv_Area, Sale_Price)) + 
  theme_bw() +
  geom_point(size = 1, alpha = 0.3) +
  geom_smooth(se = TRUE, method = "lm") +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Regression with AMES housing data")
g_lm_1
```

```{r echo = FALSE, out.width = '70%'}
g_lm_1 <- ggplot(data = ames, aes(Gr_Liv_Area, Sale_Price)) + theme_bw() +
  geom_point(size = 1, alpha = 0.3) +
  geom_smooth(se = TRUE, method = "lm") +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Regression with AMES housing data")
g_lm_1
```

]

.pull-right[

```{r eval = FALSE}
g_lm_2 <- model_1 %>% broom::augment() %>% 
ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
    theme_bw() +
    geom_point(size = 1, alpha = 0.3) +
    geom_line(aes(y = .fitted), col = KULbg) +
    scale_y_continuous(labels = scales::dollar) +
    ggtitle("Regression with AMES housing data")
g_lm_2

```


```{r echo = FALSE, out.width = '70%'}
model_1 %>% broom::augment() %>% ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
            theme_bw() +
            geom_point(size = 1, alpha = 0.3) +
            geom_line(aes(y = .fitted), col = KULbg) +
            scale_y_continuous(labels = scales::dollar) +
            ggtitle("Regression with AMES housing data")
```


]

---

class: inverse, center, middle

# Generalized Linear Models

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>

---

# Linear and Generalized Linear Models

.pull-left-alt[

.center[
<img src="img/esbjorn_GLM.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>
<br> <br> <img src="img/de_jong_GLM.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>  
]

]

.pull-right-alt[

With .hi-pink[linear regression models] `lm(.)`

- model specification

$$\begin{eqnarray*}
    \color{#FFA500}{Y} = \color{#e64173}{x}^{'}\color{#20B2AA}{\beta} + \epsilon.
\end{eqnarray*}$$

- $\epsilon$ is normally distributed with mean 0 and common variance $\sigma^2$, thus: $\color{#FFA500}{Y}$ is normal with mean $\color{#e64173}{x}^{'}\color{#20B2AA}{\beta}$ and variance $\sigma^2$

With .hi-pink[generalized linear regression models] `glm(.)`

- model specification

$$\begin{eqnarray*}
    g(E[\color{#FFA500}{Y}]) = \color{#e64173}{x}^{'}\color{#20B2AA}{\beta}.
\end{eqnarray*}$$

- $g(.)$ is the link function

- $\color{#FFA500}{Y}$ follows a distribution from the exponential family.

]


---

name: mtpl

# Motor Third Party Liability data <img src="img/pipe.png" class="title-hex"> <img src="img/dplyr.png" class="title-hex"> <img src="img/ggplot2.png" class="title-hex">

We will use the Motor Third Party Liability data set. There are 163,231 policyholders in this data set. 

The frequency of claiming (`nclaims`) and corresponding severity (`avg`, the amount paid on average per claim reported by a policyholder) are the .KULbginline[target variables] in this data set. 

Predictor variables are: 

* the exposure-to-risk, the duration of the insurance coverage (max. 1 year)
* factor variables, e.g. gender, coverage, fuel
* continuous, numeric variables, e.g. age of the policyholder, age of the car
* spatial information: postal code (in Belgium) of the municipality where the policyholder resides.

More details in [Henckaerts et al. (2018, Scandinavian Actuarial Journal)](https://katrienantonio.github.io/projects/2019/06/13/machine-learning/#data-driven) and [Henckaerts et al. (2020, North American Actuarial Journal)](https://katrienantonio.github.io/projects/2019/06/13/machine-learning/#tree-based-pricing).

---

name: mtpl

# Motor Third Party Liability data <img src="img/pipe.png" class="title-hex"> <img src="img/dplyr.png" class="title-hex"> <img src="img/ggplot2.png" class="title-hex">

You can load the data from the `data` folder as follows:
```{r eval = FALSE}
# install.packages("rstudioapi")
dir <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(dir)
mtpl_orig <- read.table('../data/PC_data.txt',
                        header = TRUE, 
                        stringsAsFactors = TRUE)
mtpl_orig <- as_tibble(mtpl_orig)
```

Alternatively, you can also go for:
```{r eval = TRUE}
# install.packages("here")
dir <- here::here()   
setwd(dir) 
mtpl_orig <- read.table('../data/PC_data.txt', 
                        header = TRUE,
                        stringsAsFactors = TRUE)
mtpl_orig <- as_tibble(mtpl_orig)
```

Some basic exploratory steps with this data follow on the next sheet.

---

name: mtpl

# Motor Third Party Liability data <img src="img/pipe.png" class="title-hex"> <img src="img/dplyr.png" class="title-hex"> <img src="img/ggplot2.png" class="title-hex">

Note that the data `mtpl_orig` uses capitals for the variable names

```{r, out.width='35%'}
mtpl_orig %>% slice(1:3) %>% select(-LONG, -LAT) %>% kable(format = 'html')
```

We change this to lower case variables, and rename `exp` to `expo`.

```{r}
mtpl <- mtpl_orig %>% rename_all(tolower) %>% rename(expo = exp)
names(mtpl)
```

---

class: clear
name: first-steps-MTPL

.pull-left[
```{r first-inspection-mtpl, eval = F}
dim(mtpl)
```

```{r , echo = F}
dim(mtpl)
```

```{r first-risk-calculations-mtpl-2, eval = F}
mtpl %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo))
```

```{r, echo = F}
mtpl %>% summarize(emp_freq = sum(nclaims) / sum(expo)) %>% kable(format = 'html')
```

```{r first-risk-calculations-mtpl-3, eval = F}
mtpl %>% 
  group_by(sex) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo))
```

```{r, echo = F}
mtpl %>% 
  group_by(sex) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo)) %>% kable(format = "html") 
```
]

.pull-right[
```{r first-graphs-mtpl, eval = F}
g <- ggplot(mtpl, aes(nclaims)) + theme_bw() + 
     geom_bar(aes(weight = expo),
              col = KULbg, fill = KULbg) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims")
g
```

```{r , echo = F, out.width = '80%'}
g <- ggplot(mtpl, aes(nclaims)) + theme_bw() + 
     geom_bar(aes(weight = expo), 
              col = KULbg, fill = KULbg) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims")
g
```

]

---


name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

To get warmed up, let's load the `mtpl` data and do some .KULbginline[basic investigations] into the variables. The idea is to get a feel for the data.

.hi-pink[Q]: you will work through the following exploratory steps.

1. Visualize the distribution of the `ageph` with a histogram.

2. For each age recorded in the data set `mtpl`: what is the total number of observations, the total exposure, and the corresponding total number of claims reported?

3. Calculate the empirical claim frequency, per unit of exposure, for each age and picture it. Discuss this figure.

4. Repeat the above for `bm`, the level occupied by the policyholder in the Belgian bonus- malus scale.

]

---

class: clear

.pull-left[

.hi-pink[Q.1] a histogram of `ageph`

```{r, out.width='70%'}
ggplot(data = mtpl, aes(ageph)) + theme_bw() +
  geom_histogram(binwidth = 2, alpha = .5,
                 col = KULbg, fill = KULbg) +
  labs(y = "Absolure frequency") +
  ggtitle("MTPL - age policyholder")
```

]

.pull-right[

.hi-pink[Q.2] for each `ageph` recorded

```{r, eval = FALSE}
mtpl %>% 
  group_by(ageph) %>% 
  summarize(tot_claims = sum(nclaims),
            tot_expo = sum(expo),
            tot_obs = n())
```

```{r, echo = FALSE}
mtpl %>% 
  group_by(ageph) %>% 
  summarize(tot_claims = sum(nclaims),
            tot_expo = sum(expo),
            tot_obs = n()) %>% 
  slice(1:7) %>% kable(format = 'html')
```

]

---

class: clear

.pull-left[

.hi-pink[Q.3] for each `ageph` recorded

```{r, eval = FALSE}
freq_by_age <- mtpl %>% 
  group_by(ageph) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo))

ggplot(data = freq_by_age,
       aes(x = ageph, y = emp_freq)) + theme_bw() +
  geom_bar(stat = 'identity', alpha = .5,
           color = KULbg, fill = KULbg) +
  ggtitle('MTPL - empirical claim freq per 
          age policyholder')
```


.hi-pink[Q.4] recycle the above instructions and replace `ageph` with `bm`

]

.pull-right[

<br>

```{r, echo = FALSE, out.width='80%'}
freq_by_age <- mtpl %>% 
  group_by(ageph) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo))

ggplot(data = freq_by_age,
       aes(x = ageph, y = emp_freq)) + theme_bw() +
  geom_bar(stat = 'identity', alpha = .5,
           color = KULbg, fill = KULbg) +
  ggtitle('MTPL - empirical claim freq per 
          age policyholder')
```

]



---

# Generalized Linear Models (GLMs) 

Modeling claim .hi-pink[frequency] and .hi-pink[severity] in the `mtpl` data set.

.pull-left[

Target variable `nclaims` (frequency)

```{r echo=FALSE, out.width='65%'}
g_freq <- ggplot(mtpl, aes(nclaims)) + theme_bw() + 
     geom_bar(aes(weight = expo), col = KULbg, 
                               fill = KULbg, alpha = .5) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims")
g_freq
```


Suitable distributions: Poisson, Negative Binomial.

]

.pull-right[

...  and `avg` (severity).

```{r echo=FALSE, out.width='65%'}
g_sev <- ggplot(mtpl, aes(x = avg)) + theme_bw() +
  geom_histogram(bins = 30, boundary = 0, color = KULbg, fill = KULbg, alpha = .5) + 
  labs(x = "claim severity") +
  xlim(c(0, 20000))
g_sev
```


Suitable distributions: log-normal, gamma.

]

---

# A Poisson GLM

.pull-left[

```{r eval=FALSE}
freq_glm_1 <- `glm`(nclaims ~ sex, offset = log(expo), 
                  `family = poisson(link = "log")`, 
                  `data = mtpl`) 
```

]

.pull-right[
  
Fit a .KULbginline[Poisson GLM], with .KULbginline[logarithmic link] function.

This implies: 

$\color{#FFA500}{Y}$ ~ Poisson, with

$$\begin{eqnarray*}
    \log(E[\color{#FFA500}{Y}]) &=& \color{#e64173}{x}^{'}\color{#20B2AA}{\beta},
\end{eqnarray*}$$

or, 

$$E[\color{#FFA500}{Y}] = \exp{(\color{#e64173}{x}^{'}\color{#20B2AA}{\beta})}.$$

Fit this model on `data = mtpl`. 
  
]

---
  
# A Poisson GLM (cont.)

.pull-left[

```{r eval=FALSE}
freq_glm_1 <- glm(`nclaims ~ sex`, `offset = log(expo)`, 
                  family = poisson(link = "log"), 
                  data = mtpl)
```

]

.pull-right[
  
Use `nclaims` as $\color{#FFA500}{Y}$. 

Use `gender` as the only (factor) variable in the linear predictor.

Include `log(expo)` as an offset term in the linear predictor.

Then, 

$$\begin{eqnarray*}
\color{#e64173}{x}^{'}\color{#20B2AA}{\beta} = \log{(\texttt{expo})}+\beta_0 + \beta_1 \mathbb{I}(\texttt{male}). \end{eqnarray*}$$

Put otherwise, 

$$\begin{eqnarray*}
E[\color{#FFA500}{Y}] = \texttt{expo} \cdot \exp{(\beta_0 + \beta_1 \mathbb{I}(\texttt{male}))} \end{eqnarray*},$$
where $\texttt{expo}$ refers to `expo` the exposure variable.
]

---

class: clear

.pull-left[

```{r eval=FALSE}
freq_glm_1 <- glm(nclaims ~ sex, `offset = log(expo)`, 
                  family = poisson(link = "log"), 
                  data = mtpl)
```

```{r eval=FALSE}
freq_glm_1 %>% broom::tidy()
```

```{r echo=FALSE}
freq_glm_1 <- glm(nclaims ~ sex, offset = log(expo), 
                  family = poisson(link = "log"), 
                  data = mtpl)
freq_glm_1 %>% broom::tidy() %>% kable(format = 'html')
```

Mind the specification of `type.predict` when using `augment` with a GLM!

```{r eval=FALSE}
freq_glm_1 %>% broom::augment(type.predict = "response")
```

```{r echo=FALSE}
freq_glm_1 %>% broom::augment(type.predict = "response") %>% slice(1:2) %>% select(nclaims, sex, .fitted) %>% kable(format = 'html')
```



]

.pull-right[

The `predict` function of a GLM object offers 3 options: `"link"`, `"response"` or `"terms"`. 

The same options hold when `augment()` is applied to a GLM object.

Let's see how the fitted values at `"response"` level are constructed:

```{r}
exp(coef(freq_glm_1)[1])
exp(coef(freq_glm_1)[1] + coef(freq_glm_1)[2])
```

Do you recognize these numbers?

Last step: 

try `freq_glm_1 %>% glance()` or `summary(freq_glm_1)` for deviances. 

]

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

You will further explore GLMs in R with the `glm(.)` function.

.hi-pink[Q]: continue with the `freq_glm_1` object that was created, you will now explicitly call the `predict()` function on this object. 

1. Verify the arguments of `predict.glm` using `? predict.glm`.

2. The help reveals the following structure `predict(.object, .newdata, type = ("..."))` where `.object` is the fitted GLM object, `.newdata` is (optionally) a data frame to look for the features used in the model, and `type` is  `"link"`, `"response"` or `"terms"`. <br> Use `predict` with `freq_glm_1` and a newly created data frame. <br> Explore the different options for `type`, and their connections. 

3. Fit a gamma GLM for `avg` (the claim severity) with log link. <br>
Use `sex` as the only variable in the model. What do you conclude?
]

---

class: clear

.pull-left[
.hi-pink[Q.1] You can access the documentation via `? predict.glm`.

.hi-pink[Q.2] You create new data frames (or tibbles) as follows

```{r eval = FALSE}
male_driver <- data.frame(expo = 1, sex = "male")
female_driver <- data.frame(expo = 1, sex = "female")
```

```{r echo = FALSE}
male_driver <- data.frame(expo = 1, sex = "male")
female_driver <- data.frame(expo = 1, sex = "female")
```

Next, you apply `predict` with the GLM object `freq_glm_1` and one of these data frames, e.g.

```{r eval = FALSE}
predict(freq_glm_1, newdata = male_driver, 
                    type = "response")
```

```{r echo = FALSE}
predict(freq_glm_1, newdata = male_driver, type = "response")
```

]

.pull-right[

.hi-pink[Q.2] Next, you apply `predict` with the GLM object `freq_glm_1` and one of these data frames, e.g.

```{r eval = FALSE}
predict(freq_glm_1, newdata = male_driver, 
                    type = "response")
```

```{r echo = FALSE}
predict(freq_glm_1, newdata = male_driver, type = "response")
```

At the level of the linear predictor: 

```{r eval = FALSE}
predict(freq_glm_1, newdata = male_driver, 
                    type = "link")
```

```{r echo = FALSE}
predict(freq_glm_1, newdata = male_driver, type = "link")
```

```{r eval = FALSE}
exp(predict(freq_glm_1, newdata = male_driver, 
                        type = "link"))
```

```{r echo = FALSE}
exp(predict(freq_glm_1, newdata = male_driver, 
                        type = "link"))
```

]

---

class: clear

.hi-pink[Q.3] For the gamma regression model

```{r eval=FALSE}
sev_glm_1 <- glm(avg ~ sex, family = Gamma(link = "log"), data = mtpl)
sev_glm_1
```

```{r echo=FALSE}
sev_glm_1 <- glm(avg ~ sex, family = Gamma(link = "log"), data = mtpl)
sev_glm_1
```

---

class: inverse, center, middle

# Generalized Additive Models with {mgcv}

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>


---

# Generalized Additive Models (GAMs)

.pull-left-alt[

.center[
<img src="img/GAM_Hastie_Tibshirani.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>
<br> <br> <img src="img/GAM_Wood.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>  
]

]

.pull-right-alt[

With .hi-pink[GLMs] `glm(.)`

- transformation of the mean modelled with a linear predictor $$\color{#e64173}{x}^{'}\color{#20B2AA}{\beta}$$

- not well suited for continuous risk factors that relate to the response in
a non-linear way.

With .hi-pink[Generalized Additive Models (GAMs)] 

- the predictor allows for smooth effects of continuous risk factors and spatial covariates, next to the linear terms, e.g.

$$\color{#e64173}{x}^{'}\color{#20B2AA}{\beta}+\sum_j f_j(x_j) + f(\texttt{lat}, \texttt{long})$$

- predictor is still additive

- preferred R package is {mgcv} by Simon Wood.


]

---

# A Poisson GAM

.pull-left[
We continue working with `mtpl` and now focus on `ageph`.

```{r echo=FALSE, out.width='75%'}
mtpl %>% group_by(ageph) %>% 
  summarize(emp_freq = sum(nclaims) / sum(expo)) %>% 
  ggplot(aes(x = ageph, y = emp_freq)) + theme_bw() +
  geom_point(color = KULbg)
```

]

.pull-right[

We will now explore .hi-pink[four different model specifications]: 

1. `ageph` as linear effect in `glm`

2. `ageph` as factor variable in `glm`

3. `ageph` split manually into bins using `cut`, then used as factor in `glm`

4. a smooth effect of `ageph` in `mgcv::gam`.

Let's go!

Grid of observed `ageph` values

```{r}
a <- min(mtpl$ageph):max(mtpl$ageph)
```

]

---

class: clear

.pull-left[

.hi-pink[Model 1]: linear effect of `ageph`

```{r eval=FALSE}
freq_glm_age <- glm(nclaims ~ `ageph`, 
                    offset = log(expo), 
                    data = mtpl, 
                    family = poisson(link = "log"))
pred_glm_age <- predict(freq_glm_age, 
          newdata = data.frame(ageph = a, expo = 1), 
          `type = "terms"`, se.fit = TRUE)
b_glm_age <- pred_glm_age$fit
l_glm_age <- pred_glm_age$fit 
                  - qnorm(0.975)*pred_glm_age$se.fit
u_glm_age <- pred_glm_age$fit 
                  + qnorm(0.975)*pred_glm_age$se.fit
df <- data.frame(a, b_glm_age, l_glm_age, u_glm_age)
```


```{r echo=FALSE}
freq_glm_age <- glm(nclaims ~ ageph, offset = log(expo), data = mtpl, family = poisson(link = "log"))
pred_glm_age <- predict(freq_glm_age, newdata = data.frame(ageph = a, expo = 1), type = "terms", se.fit = TRUE)
b_glm_age <- pred_glm_age$fit
l_glm_age <- pred_glm_age$fit - qnorm(0.975)*pred_glm_age$se.fit
u_glm_age <- pred_glm_age$fit + qnorm(0.975)*pred_glm_age$se.fit
df <- data.frame(a, b_glm_age, l_glm_age, u_glm_age)
```

]

.pull-right[

```{r echo=FALSE, out.width='100%'}
p_glm_age <- ggplot(df, aes(x = a)) + ylim(-0.5, 1)
p_glm_age <- p_glm_age + geom_line(aes(a, b_glm_age), size = 1, col = KULbg)   
p_glm_age <- p_glm_age + geom_line(aes(a, u_glm_age), size = 0.5, linetype = 2, col = KULbg) + geom_line(aes(a, l_glm_age), size = 0.5, linetype = 2, col = KULbg)
p_glm_age <- p_glm_age + xlab("ageph") + ylab("fit") + theme_bw()
p_glm_age
```

]
---

class: clear

.pull-left[

.hi-pink[Model 2]: `ageph` as factor variable in `glm`

```{r eval=FALSE}
freq_glm_age_f <- glm(nclaims ~ `as.factor(ageph)`, 
                    offset = log(expo), 
                    data = mtpl, 
                    family = poisson(link = "log"))
pred_glm_age_f <- predict(freq_glm_age_f, 
          newdata = data.frame(ageph = a, expo = 1), 
          `type = "terms"`, se.fit = TRUE)
b_glm_age_f <- pred_glm_age_f$fit
l_glm_age_f <- pred_glm_age_f$fit 
                  - qnorm(0.975)*pred_glm_age_f$se.fit
u_glm_age_f <- pred_glm_age_f$fit 
                  + qnorm(0.975)*pred_glm_age_f$se.fit
df <- data.frame(a, b_glm_age_f, 
                    l_glm_age_f, u_glm_age_f)
```


```{r echo=FALSE}
freq_glm_age_f <- glm(nclaims ~ as.factor(ageph), offset = log(expo), data = mtpl, family = poisson(link = "log"))
pred_glm_age_f <- predict(freq_glm_age_f, newdata = data.frame(ageph = a, expo = 1), type = "terms", se.fit = TRUE)
b_glm_age_f <- pred_glm_age_f$fit
l_glm_age_f <- pred_glm_age_f$fit - 
               qnorm(0.975)*pred_glm_age_f$se.fit
u_glm_age_f <- pred_glm_age_f$fit + 
               qnorm(0.975)*pred_glm_age_f$se.fit
df <- data.frame(a, b_glm_age_f, 
                    l_glm_age_f, u_glm_age_f)
```

]

.pull-right[

```{r echo=FALSE, out.width='100%'}
p_glm_age_f <- ggplot(df, aes(x = a)) + ylim(-0.5, 1)
p_glm_age_f <- p_glm_age_f + geom_line(aes(a, b_glm_age_f), size = 1, col = KULbg)   
p_glm_age_f <- p_glm_age_f + geom_line(aes(a, u_glm_age_f), size = 0.5, linetype = 2, col = KULbg) + geom_line(aes(a, l_glm_age_f), size = 0.5, linetype = 2, col = KULbg)
p_glm_age_f <- p_glm_age_f + xlab("ageph") + ylab("fit") + theme_bw()
p_glm_age_f
```

]

---

class: clear

.pull-left[

.hi-pink[Model 3]: `ageph` split into 5-year bins and then used in `glm`

```{r eval=FALSE}
level <- seq(min(mtpl$ageph), max(mtpl$ageph), by = 5) #<<
freq_glm_age_c <- glm(nclaims ~ `cut(ageph, level)`, 
                    offset = log(expo), 
                    data = mtpl, 
                    family = poisson(link = "log"))
pred_glm_age_c <- predict(freq_glm_age_c, 
          newdata = data.frame(ageph = a, expo = 1), 
          `type = "terms"`, se.fit = TRUE)
b_glm_age_c <- pred_glm_age_c$fit
l_glm_age_c <- pred_glm_age_c$fit 
                  - qnorm(0.975)*pred_glm_age_c$se.fit
u_glm_age_c <- pred_glm_age_c$fit 
                  + qnorm(0.975)*pred_glm_age_c$se.fit
df <- data.frame(a, b_glm_age_c, 
                    l_glm_age_c, u_glm_age_c)
```


```{r echo=FALSE}
level <- seq(min(mtpl$ageph), max(mtpl$ageph), by = 5)
freq_glm_age_c <- glm(nclaims ~ cut(ageph, level), offset = log(expo), data = mtpl, family = poisson(link = "log"))
pred_glm_age_c <- predict(freq_glm_age_c, newdata = data.frame(ageph = a, expo = 1), type = "terms", se.fit = TRUE)
b_glm_age_c <- pred_glm_age_c$fit
l_glm_age_c <- pred_glm_age_c$fit - 
               qnorm(0.975)*pred_glm_age_c$se.fit
u_glm_age_c <- pred_glm_age_c$fit + 
               qnorm(0.975)*pred_glm_age_c$se.fit
df <- data.frame(a, b_glm_age_c, 
                    l_glm_age_c, u_glm_age_c)
```

]

.pull-right[

```{r echo=FALSE, out.width='100%'}
p_glm_age_c <- ggplot(df, aes(x = a)) + ylim(-0.5, 1)
p_glm_age_c <- p_glm_age_c + geom_line(aes(a, b_glm_age_c), size = 1, col = KULbg)   
p_glm_age_c <- p_glm_age_c + geom_line(aes(a, u_glm_age_c), size = 0.5, linetype = 2, col = KULbg) + geom_line(aes(a, l_glm_age_c), size = 0.5, linetype = 2, col = KULbg)
p_glm_age_c <- p_glm_age_c + xlab("ageph") + ylab("fit") + theme_bw()
p_glm_age_c
```

]

---

class: clear

.pull-left[

.hi-pink[Model 4]: smooth effect of `ageph` in `mgcv::gam`

```{r eval=FALSE}
library(mgcv)
freq_gam_age <- gam(nclaims ~ `s(ageph)`, 
                    offset = log(expo), 
                    data = mtpl, 
                    family = poisson(link = "log"))
pred_gam_age <- predict(freq_gam_age, 
          newdata = data.frame(ageph = a, expo = 1), 
          `type = "terms"`, se.fit = TRUE)
b_gam_age <- pred_gam_age$fit
l_gam_age <- pred_gam_age$fit -
                  qnorm(0.975)*pred_gam_age$se.fit
u_gam_age <- pred_gam_age$fit +
                  qnorm(0.975)*pred_gam_age$se.fit
df <- data.frame(a, b_gam_age, 
                    l_gam_age, u_gam_age)
```


```{r echo=FALSE}
library(mgcv)
freq_gam_age <- gam(nclaims ~ s(ageph), 
                    offset = log(expo), 
                    data = mtpl, 
                    family = poisson(link = "log"))
pred_gam_age <- predict(freq_gam_age, 
          newdata = data.frame(ageph = a, expo = 1), 
          type = "terms", se.fit = TRUE)
b_gam_age <- pred_gam_age$fit
l_gam_age <- pred_gam_age$fit -
                  qnorm(0.975)*pred_gam_age$se.fit
u_gam_age <- pred_gam_age$fit +
                  qnorm(0.975)*pred_gam_age$se.fit
df <- data.frame(a, b_gam_age, 
                    l_gam_age, u_gam_age)
```

]

.pull-right[

```{r echo=FALSE, out.width='100%'}
p_gam_age <- ggplot(df, aes(x = a)) + ylim(-0.5, 1)
p_gam_age <- p_gam_age + geom_line(aes(a, b_gam_age), size = 1, col = KULbg)   
p_gam_age <- p_gam_age + geom_line(aes(a, u_gam_age), size = 0.5, linetype = 2, col = KULbg) + geom_line(aes(a, l_gam_age), size = 0.5, linetype = 2, col = KULbg)
p_gam_age <- p_gam_age + xlab("ageph") + ylab("fit") + theme_bw()
p_gam_age
```

]

---

class: clear

.hi-pink[Model 4] (revisited): picture smooth effect of `ageph` in `mgcv::gam` with built-in `plot`.

```{r, out.width='35%', out.height='10%'}
library(mgcv)
freq_gam <- gam(nclaims ~ s(ageph), offset = log(expo), family = poisson(link = "log"), data = mtpl)
plot(freq_gam, scheme = 4)
```

---

# More on GAMs

.pull-left[
So, a GAM is a GLM where the linear predictor depends on .hi-pink[smooth functions] of covariates.

Consider a GAM with the following predictor:

$$\color{#e64173}{x}^{'}\color{#20B2AA}{\beta}+f_j(x_j).$$

GAMs use .hi-pink[basis functions] to estimate the smooth effect $f_j(.)$

$$f_j(x_j) = \sum_{m=1}^M \beta_{jm} b_{jm}(x_j),$$

where the $b_{jm}(x)$ are known basis functions and $\beta_{jm}$ are coefficients that have to be estimated.
]

.pull-right[

GAMs avoid overfitting by adding a .hi-pink[wiggliness penalty] to the likelihood

$$\int \left( f_j(x)'' \right)^2 = \beta_j^t \mathbf{S}_j\beta_j.$$
GAMs then balance goodness-of-fit and wiggliness via

$$\log \mathcal{L}(\beta,\beta_j) - \lambda_j \cdot \beta_j^t \mathbf{S}_j\beta_j,$$

with $\lambda_j$ the .hi-pink[smoothing parameter].

The smoothing parameter $\lambda_j$ controls the trade-off between fit & smoothness.
]

---

class: clear

Let's run some experiments to illustrate the effect of the smoothing parameter (   `sp = .`), the number (`k = .`) and type of basis functions (`bs = .`). We use the `mcycle` data from {MASS}.

```{r, echo = FALSE, fig.width=12, fig.height = 5}
KULbg <- "#116E8A"

# number 1
library(MASS)
bias_model <- gam(accel ~ s(times, sp = 0, k = 2), data = mcycle)
mcycle$predictions <- predict(bias_model, mcycle)
p_1 <- ggplot(mcycle, aes(times, accel)) + theme_bw() +
  geom_point(alpha = .3) +
  geom_line(aes(times, predictions), size = 1.0, color = KULbg) +
  theme(axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  scale_x_continuous(expand = c(0, 0)) + 
  ggtitle("sp = 0 and k = 2")
# number 2
bias_model <- gam(accel ~ s(times, sp = 0, k = 5), data = mcycle)
mcycle$predictions <- predict(bias_model, mcycle)
p_2 <- ggplot(mcycle, aes(times, accel)) + theme_bw() +
  geom_point(alpha = .3) +
  geom_line(aes(times, predictions), size = 1.0, color = KULbg) +
  theme(axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  scale_x_continuous(expand = c(0, 0)) +
  ggtitle("sp = 0 and k = 5")
# number 3
bias_model <- gam(accel ~ s(times, sp = 0, k = 55), data = mcycle)
mcycle$predictions <- predict(bias_model, mcycle)
p_3 <- ggplot(mcycle, aes(times, accel)) + theme_bw() +
  geom_point(alpha = .3) +
  geom_line(aes(times, predictions), size = 1.0, color = KULbg) +
  theme(axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  scale_x_continuous(expand = c(0, 0)) + 
  ggtitle("sp = 0 and k = 15")
# number 4
library(MASS)
bias_model <- gam(accel ~ s(times), data = mcycle)
mcycle$predictions <- predict(bias_model, mcycle)
p_4 <- ggplot(mcycle, aes(times, accel)) + theme_bw() + 
  geom_point(alpha = .3) +
  geom_line(aes(times, predictions), size = 1.0, color = KULbg) +
  theme(axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  scale_x_continuous(expand = c(0, 0)) + 
  ggtitle("optimal sp and default k")
# number 5
bias_model <- gam(accel ~ s(times, sp = 3), data = mcycle)
mcycle$predictions <- predict(bias_model, mcycle)
p_5 <- ggplot(mcycle, aes(times, accel)) + theme_bw() +
  geom_point(alpha = .3) +
  geom_line(aes(times, predictions), size = 1.0, color = KULbg) +
  theme(axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  scale_x_continuous(expand = c(0, 0)) + 
  ggtitle("sp = 3 and default k")
# number 6
bias_model <- gam(accel ~ s(times, sp = 20), data = mcycle)
mcycle$predictions <- predict(bias_model, mcycle)
p_6 <- ggplot(mcycle, aes(times, accel)) + theme_bw() +
  geom_point(alpha = .3) +
  geom_line(aes(times, predictions), size = 1.0, color = KULbg) +
  theme(axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  scale_x_continuous(expand = c(0, 0)) +
  ggtitle("sp = 10 and default k")

gridExtra::grid.arrange(p_1, p_2, p_3, p_4, p_5, p_6, nrow = 2)
```


---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

You will further explore GAMs in R with the `gam(.)` function from the {mgcv} package.

.hi-pink[Q]: you will combine insights from building `glm` as well as `gam` objects by working through the following coding steps. 

1. Fit a `gam` including some factor variables as well as a smooth effect of `ageph` and `bm`. Visualize the fitted smooth effects.

2. Specify risk profiles of drivers. Calculate their expected annual claim frequency from the constructed `gam`.

3. Explain (in words) which profiles would represent high vs low risk according to the constructed model. 
]

---

class: clear

.pull-left[

.hi-pink[Q.1] examine the following `gam` fit

```{r}
freq_gam_2 <- gam(nclaims ~ sex + fuel + use + 
                    s(ageph) + s(bm),
                  offset = log(expo), data = mtpl,
                  family = poisson(link = "log"))
```

```{r}
summary(freq_gam_2)
```


]

.pull-right[

```{r, out.width='45%'}
plot(freq_gam_2, select = 1)
```


```{r, out.width='45%'}
plot(freq_gam_2, select = 2)
```

]

---

class: clear

.pull-left[

.hi-pink[Q.2] define some risk profiles 

```{r eval = FALSE}
drivers <- data.frame(expo = c(1, 1, 1), 
                      sex = c("female", "female", "female"), 
                      fuel = c("diesel", "diesel", "diesel"), 
                      use = c("private", "private", "private"), 
                      ageph = c(18, 45, 65), bm = c(20, 5, 0))
drivers 
```

```{r echo = FALSE}
drivers <- data.frame(expo = c(1, 1, 1), sex = c("female", "female", "female"), fuel = c("diesel", "diesel", "diesel"), use = c("private", "private", "private"), ageph = c(18, 45, 65), bm = c(20, 5, 0))
drivers %>% kable(format = 'html')
```
]

.pull-right[
Now, you predict the annual expected claim frequency for these profiles. 

```{r eval = FALSE}
predict(freq_gam_2, newdata = drivers, 
        type = "response")
```

```{r echo = FALSE}
predict(freq_gam_2, newdata = drivers, 
        type = "response") %>% kable(format = 'html')
```

]


---

class: inverse, center, middle

# Regularized (G)LMs met {glmnet}

<html><div style='float:left'></div><hr color='#FAFAFA' size=1px width=796px></html>


---


# Statistical learning with sparsity

.pull-left-alt[

.center[
<img src="img/sparsity_book.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>
<br> <br> <img src="img/boehmke_greenwell.jpg" alt="Drawing" style="width: 150px; height: 220px;"/>  
]

]

.pull-right-alt[

Why?

* Sort through the mass of information and bring it down to .hi-pink[its bare essentials].

* One form of simplicity is .hi-pink[sparsity].

* Only a relatively small number of predictors play a role.

How? &nbsp; &nbsp; .KULbginline[Automatic feature selection!]

* Fit a model with all *p* predictors, but constrain or .hi-pink[regularize] the coefficient estimates. 

* Shrinking the coeffcient estimates can signifcantly reduce their variance. 

* Some types of shrinkage put some of the coefficients .KULbginline[exactly equal to zero]!

]

---

# Ridge and lasso (least squares) regression

.pull-left[
.KULbginline[Ridge] considers the least-squares optimization problem

$$\begin{eqnarray*}
\min_{\beta_0,\boldsymbol{\beta}} \ \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2 = \min_{\beta_0,\boldsymbol{\beta}} \ \text{RSS}
\end{eqnarray*}$$

subject to a .hi-pink[budget constraint]

$$\begin{eqnarray*}
\sum_{j=1}^p \beta_j^2 \leq \color{#e64173}{t},
\end{eqnarray*}$$

i.e. an $\ell_2$ penalty.

Shrinks the coefficient estimates (not the intercept) to zero. 


]

.pull-right[
.KULbginline[Lasso] considers the least-squares optimization problem

$$\begin{eqnarray*}
\min_{\beta_0,\boldsymbol{\beta}} \ \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2 = \min_{\beta_0,\boldsymbol{\beta}} \ \text{RSS}
\end{eqnarray*}$$

subject to a .hi-pink[budget constraint]

$$\begin{eqnarray*}
\sum_{j=1}^p |\beta_j| \leq \color{#e64173}{t},
\end{eqnarray*}$$

i.e. an $\ell_1$ penalty.

Shrinks the coefficient estimates (not the intercept) to zero and does variable selection!

Lasso is for .hi-pink[L]east .hi-pink[a]bsolute .hi-pink[s]hrinkage and .hi-pink[s]election .hi-pink[o]perator.


]

---

# Ridge and lasso (least squares) regression (cont.)

.pull-left[

The .KULbginline[dual problem] formulation:

* with ridge penalty: 

$$\begin{eqnarray*}
\min_{\beta_0,\boldsymbol{\beta}} \ \text{RSS} + \color{#e64173}{\lambda} \sum_{j=1}^p \beta_j^2
\end{eqnarray*}$$

* with lasso penalty:
$$\begin{eqnarray*}
\min_{\beta_0,\boldsymbol{\beta}} \ \text{RSS}+\color{#e64173}{\lambda} \sum_{j=1}^p |\beta_j|.
\end{eqnarray*}$$

$\color{#e64173}{\lambda}$ is a tuning parameter; use resampling methods to pick a value!

Both ridge and lasso require .hi-pink[centering and scaling] of the features. 

]

.pull-right[

.center[
```{r out.width = '100%', echo=FALSE}
knitr::include_graphics("img/6.7_ISL.png")
```
]

Ellipses (around least-squares solution) represent regions of constant RSS. 

Lasso budget on the left and ridge budget on the right.

Source: James et al. (2013) on [An introduction to statistical learning](http://faculty.marshall.usc.edu/gareth-james/ISL/).

]

---

# Regularized GLMs 

.pull-left[

We now focus on generalizations of linear models and the lasso.

Minimize

$$\begin{eqnarray*}
\min_{\beta_0,\ \beta} -\frac{1}{n} \mathcal{L}(\beta_0,\ \beta;\ y,\ X)+\lambda \|\boldsymbol{\beta}\|_1.
\end{eqnarray*}$$

Here: 

* $\mathcal{L}$ is the log-likelihood of a GLM.

* $n$ is the sample size

* $\|\boldsymbol{\beta}\|_1 = \sum_{j=1}^p \beta_j$ the $\ell_1$ penalty.

What happens if: 

* $\lambda \to 0$?

* $\lambda \to \infty$?

]

.pull-right[

.center[
```{r out.width = '100%', echo=FALSE}
knitr::include_graphics("img/lasso_path.png")
```
]

The R package {glmnet} fits linear, logistic and multinomial, Poisson, and Cox regression models.

]
---

# Fit a GLM with lasso regularization in {glmnet}

{glmnet} is a package that fits a generalized linear model via penalized maximum likelihood.

Main function call (with a selection of arguments, see `? glmnet` for a complete list)

```{r eval = FALSE}
fit <- glmnet(x, y, family = ., alpha = ., weights = ., offset = ., nlambda = ., standardize = ., intercept = .)
```

where

* `x` is the input matrix and `y` is the response variable
* `family` the response type, e.g. `family = poisson`
* `weights` and `offset` 
* `nlambda` is the number of $\lambda$ values, default is 100
* `standardize` should `x` be standardized prior to fitting the model sequence?
* `intercept` should incercept be fitted?
* `alpha` a value between 0 and 1, such that the penalty becomes
$$\begin{eqnarray*}
\lambda P_{\alpha}(\boldsymbol{\beta}) = \lambda \cdot \sum_{j=1}^p \left\{\frac{(1-\alpha)}{2}\beta_j^2 + \alpha |\beta_j|\right\}.
\end{eqnarray*}$$
Thus, with $\alpha = 1$ the lasso penalty and $\alpha = 0$ the ridge penalty results.

---

# A first example of {glmnet}

.pull-left[

Following [the vignette](https://web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf) we start with penalized linear regression

```{r eval=TRUE}
library(glmnet)
data(QuickStartExample)
```

This example loads an input matrix `x` and vector `y` of outcomes. The input matrix `x` is not standardized yet (check this!).

We calibrate a lasso linear regression model

```{r eval=FALSE}
fit <- glmnet(x, y, family = "gaussian", 
              `alpha = 1`, standardize = TRUE, 
              intercept = TRUE)
summary(fit)
```


]

.pull-right[

```{r echo=FALSE, out.width='85%'}
library(glmnet)
data(QuickStartExample)
fit <- glmnet(x, y, family = "gaussian", alpha = 1, standardize = TRUE, intercept = TRUE)
```

Note that the formula notation `y ~ x` can not be used with `glmnet`.

Some `tidy` instructions are available for `glmnet` objects (but not all), e.g.

```{r eval=FALSE}
library(broom)
tidy(fit)
```

```{r echo=FALSE}
library(knitr)
library(broom)
tidy(fit) %>% slice(1:5) %>% kable(format = 'html')
```

]

---

class: clear

.pull-left[

```{r eval=FALSE, out.width='100%'}
plot(fit, `label = TRUE`)
```

```{r echo=FALSE, out.width='100%'}
plot(fit, label = TRUE)
```

]

.pull-right[

```{r eval=FALSE, out.width='100%'}
plot(fit, label = TRUE, `xvar = 'lambda'`)
```

```{r echo=FALSE, out.width='100%'}
plot(fit, label = TRUE, xvar = 'lambda')
```

]

---

class: clear

.pull-left[

```{r, eval=FALSE, out.width='100%'}
plot(fit, `xvar = 'dev'`, label = TRUE)
```

```{r, echo=FALSE, out.width='100%'}
plot(fit, xvar = 'dev', label = TRUE)
```

]

.pull-right[

```{r, eval=TRUE}
print(fit) 
```

]

---

class: clear

.pull-left[

Get estimated coefficients for handpicked value

```{r eval = FALSE}
coef(fit, `s = 0.1`)
```

```{r echo = FALSE}
coef(fit, s = 0.1)
```


]

.pull-right[

`glmnet` returns a sequence of models for the users to choose from, i.e. a model for every `lambda`. 

How do we select the most appropriate model? 

Use cross-validation to pick a `lambda` value. The default is 10-folds cross-validation.

```{r eval=FALSE}
cv_fit <- cv.glmnet(x, y)
```

```{r echo=FALSE}
library(glmnet)
data(QuickStartExample)
cv_fit <- cv.glmnet(x, y)
```

We can pick the `lambda` that minimizes the cross-validation error. 

```{r eval=TRUE}
cv_fit$lambda.min
```

Or we use the one-standard-error-rule. 

```{r eval=TRUE}
cv_fit$lambda.1se
```

]

---

class: clear

We plot the cross-validation error for the inspected grid of `lambda` values. 

```{r eval=FALSE}
plot(cv_fit)
```

```{r echo=FALSE, out.height= '40%', out.width='45%'}
plot(cv_fit)
```

---

class: clear

.pull-left[

For the selected `lambda` (via `cv_fit$lambda.min`) we inspect which parameters are non-zero (on the right).

Now, compare this to the selected variables obtained via `cv_fit$lambda.1se`.

]

.pull-right[

```{r eval = FALSE}
coef(fit, s = cv_fit$lambda.min)
```

```{r echo = FALSE}
coef(fit, s = cv_fit$lambda.min)
```

]

---

class: clear

.pull-left[

The variables `V1`, `V3`, `V5-8`, `V11`, `V14` and `V20` are selected in the regression model. 

However, the corresponding estimates (on the left) are biased, and shrunk to zero. 

To remove this bias, we refit the model, only using the selected variables. 

```{r eval = FALSE}
subset <- data.frame(y = y, V1 = x[, 1], V3 = x[, 3], 
                     V5 = x[, 5], V6 = x[, 6], 
                     V7 = x[, 7], V8 = x[, 8], 
                     V11 = x[, 11], V14 = x[, 14], 
                     V20 = x[, 20])
final_model <- lm(y ~ V1 + V3 + V5 + V6 + V7 + V8 + 
                      V11 + V14 + V20, data = subset)
final_model %>% broom::tidy()
```

What is your judgement about `V7` (see coefficients on the right)?

]

.pull-right[

What do you observe when comparing the estimates below with those shown on the previous sheet?

```{r echo = FALSE}
library(glmnet)
data(QuickStartExample)
col <- c(1, 3, 5:8, 11, 14, 20)
subset <- data.frame(y = y, V1 = x[, 1], V3 = x[, 3], V5 = x[, 5], V6 = x[, 6], V7 = x[, 7], V8 = x[, 8], V11 = x[, 11], V14 = x[, 14], V20 = x[, 20])
final_model <- lm(y ~ V1 + V3 + V5 + V6 + V7 + V8 + V11 + V14 + V20, data = subset)
final_model %>% broom::tidy() %>% kable(format = 'html')
```
]

---

# {glmnet} and the MTPL data set

.pull-left[

Next, we fit a .KULbginline[Poisson regression model with lasso penalty] on the `mtpl` data set. 

The regularization penalty helps us to select the interesting features from the data set. 

`glmnet` requires the features as input matrix `x` and the target as a vector `y`.

Recall: 

* `mtpl` has .hi-pink[continuous] features (e.g. `ageph`, `bm`, `power`)

* `mtpl` has .hi-pink[factor] variables with .hi-pink[two levels] (e.g. `sex`, `fleet`)

* but also factor variables with .hi-pink[more than 2 levels] (`coverage`)

]

.pull-right[

Consider different types of coding factor variables.

Apply the `contrasts` function to the variable `coverage`

```{r eval = TRUE}
map(mtpl[, c("coverage")], contrasts, 
    contrasts = FALSE)
```

```{r eval = TRUE}
map(mtpl[, c("coverage")], contrasts, 
    contrasts = TRUE)
```

What's the difference?
]

---

# {glmnet} and the MTPL data set (cont.)

We construct the input matrix for `glmnet`.

```{r, eval = FALSE}
y <- mtpl$nclaims #<<

x <- model.matrix( ~ coverage + fuel + use + fleet + sex + ageph + bm +
                     agec + power, data = mtpl, 
                   contrasts.arg = map(mtpl[, c("coverage")], contrasts, 
                                       contrasts = FALSE))[,-1]

x[1:10,]
```

Put the response or outcome variable in `y`. 

In the `mtpl` data set we build a Poisson model for `nclaims`. 

---

# {glmnet} and the MTPL data set (cont.)

We construct the input matrix for `glmnet`.

```{r, eval = FALSE}
y <- mtpl$nclaims

x <- `model.matrix`( ~ coverage + fuel + use + fleet + sex + ageph + bm +
                     agec + power, data = mtpl, 
                     `contrasts.arg = map(mtpl[, c("coverage")], contrasts,` 
                                      `contrasts = FALSE)`)[,-1]

```

Use `model.matrix` to create the input matrix `x`.

We code the factor variable `coverage` with one-hot-encoding. Here, three dummy variables will be created for the three levels of `coverage`. 

The other factor variables `fuel`, `use`, `fleet`, `sex` are dummy coded, with one dummy variable. 

---

# {glmnet} and the MTPL data set (cont.)

We construct the input matrix for `glmnet`.

```{r, eval = FALSE}
y <- mtpl$nclaims

x <- model.matrix( ~ coverage + fuel + use + fleet + sex + ageph + bm +
                     agec + power, data = mtpl, 
                     contrasts.arg = map(mtpl[, c("coverage")], contrasts, 
                                       contrasts = FALSE))`[,-1]`

```

Use `model.matrix` to create the input matrix `x`.

We remove the first column, representing the intercept, from the `model.matrix`. 

---

# {glmnet} and the MTPL data set (cont.)

Let's check the input matrix `x`

```{r, echo = FALSE}
y <- mtpl$nclaims

x <- model.matrix( ~ coverage + fuel + use + fleet + sex + ageph + bm +
                     agec + power, data = mtpl, 
                     contrasts.arg = map(mtpl[, c("coverage")], contrasts, 
                                       contrasts = FALSE))[,-1]

x[1:6,]
```

You are now ready to fit a regularized Poisson GLM for `y` with input `x`. 

Let's go!

---

name: yourturn
class: clear

.left-column[

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

## <i class="fa fa-edit"></i> <br> Your turn


]

.right-column[

You will fit a regularized Poisson GLM on the `mtpl` data with the {glmnet} package.

.hi-pink[Q]: using the constructed `y` and `x` 

1. Fit a `glmnet` with lasso penalty and store the fitted object in `mtpl_glmnet`. Use the following arguments `family = "poisson", offset = ___`.

2. Display the order of the variables and their names via `row.names(mtpl_glmnet$beta)`.

3. Plot the solutions path. Pick a meaningful value for `lambda` via cross-validation.

4. Which variables are selected in the lasso model? As a last step, you will fit a Poisson GLM with the selected variables. What do you see?

5. List some pros and cons of the above strategy.
]

---

class: clear

.pull-left[

.hi-pink[Q.1] fit a regularized Poisson GLM

```{r eval = FALSE}
alpha <- 1 # for lasso penalty
mtpl_glmnet <- glmnet(x = x, y = y, 
                      family = "poisson", 
                      offset = log(mtpl$expo), 
                      alpha = alpha, 
                      standardize = TRUE, 
                      intercept = TRUE)
```

```{r echo = FALSE}
y <- mtpl$nclaims

x <- model.matrix( ~ coverage + fuel + use + fleet + sex + ageph + bm +
                     agec + power, data = mtpl, 
                     contrasts.arg = map(mtpl[, c("coverage")], contrasts, 
                                       contrasts = FALSE))[,-1]
alpha <- 1 # for lasso penalty
mtpl_glmnet <- glmnet(x = x, y = y, 
                      family = "poisson", 
                      offset = log(mtpl$expo), 
                      alpha = alpha, 
                      standardize = TRUE, 
                      intercept = TRUE)
```

.hi-pink[Q.2] display the variables via

```{r}
row.names(mtpl_glmnet$beta) 
```

]

.pull-right[

.hi-pink[Q.3] plot the solutions path

```{r eval = TRUE, out.width='85%'}
plot(mtpl_glmnet, xvar = 'lambda', label = TRUE)  
```


]

---

class: clear

.pull-left[

.hi-pink[Q.3] pick a value for `lambda`

```{r eval = TRUE, out.width='45%'}
set.seed(123)
fold_id <- sample(rep(1:10, length.out = nrow(mtpl)), 
                  nrow(mtpl))
mtpl_glmnet_cv <- cv.glmnet(x, y, family = "poisson", 
                            alpha = alpha, 
                            nfolds = 10, 
                            foldid = fold_id, 
                            type.measure = "deviance", 
                            standardize = TRUE, 
                            intercept = TRUE)
plot(mtpl_glmnet_cv)
```


]

.pull-right[

```{r eval = TRUE}
coef(mtpl_glmnet_cv, s = "lambda.min")
```

]

---

class: clear

.pull-left[

.hi-pink[Q.3] pick a value for `lambda`

```{r eval = TRUE, out.width='45%'}
set.seed(123)
fold_id <- sample(rep(1:10, length.out = nrow(mtpl)), 
                  nrow(mtpl))
mtpl_glmnet_cv <- cv.glmnet(x, y, family = "poisson", 
                            alpha = alpha, 
                            nfolds = 10, 
                            foldid = fold_id,
                            type.measure = "deviance", 
                            standardize = TRUE, 
                            intercept = TRUE)
plot(mtpl_glmnet_cv)
```


]

.pull-right[

```{r eval = TRUE}
coef(mtpl_glmnet_cv, s = "lambda.1se")
```

]

---

class: clear

.pull-left[
.hi-pink[Q.4] refit the models using only the selected features

```{r eval = TRUE}
mtpl$coverage <- relevel(mtpl$coverage, "PO")
mtpl_formula_refit <- nclaims ~ 1 + coverage + 
                      fuel + use + fleet + sex + 
                      ageph + bm + agec + power
mtpl_glm_refit <- glm(mtpl_formula_refit, 
                      data = mtpl, 
                      offset = log(mtpl$expo), 
                      family = poisson())
```

]

.pull-right[

The selection obtained via `lambda.min`

```{r echo = FALSE}
mtpl$coverage <- relevel(mtpl$coverage, "PO")
mtpl_formula_refit <- nclaims ~ 1 + coverage + fuel + use + fleet + sex + ageph + bm + agec + power
mtpl_glm_refit <- glm(mtpl_formula_refit, data = mtpl, offset = log(mtpl$expo), family = poisson())
mtpl_glm_refit %>% broom::tidy() %>% kable(format = 'html')
```

]

---

class: clear

.pull-left[

.hi-pink[Q.4] refit the models using only the selected features

```{r eval = TRUE}
mtpl_formula_refit_2 <- nclaims ~ 1 + ageph + bm 
mtpl_glm_refit_2 <- glm(mtpl_formula_refit_2, 
                        data = mtpl, 
                        offset = log(mtpl$expo), 
                        family = poisson())
```

]

.pull-right[

The selection obtained via `lambda.1se`

```{r echo = FALSE}
mtpl_formula_refi_2t <- nclaims ~ 1 + ageph + bm 
mtpl_glm_refit_2 <- glm(mtpl_formula_refit_2, data = mtpl, offset = log(mtpl$expo), family = poisson())
mtpl_glm_refit_2 %>% broom::tidy() %>% kable(format = 'html')
```

]

---
name: wrap-up

# Thanks!  <img src="img/xaringan.png" class="title-hex">

<br>
<br>
<br>
<br>

Slides created with the R package [xaringan](https://github.com/yihui/xaringan).
<br> <br> <br>
Course material available via 
<br>
`r fa(name = "github", fill = KULbg)` https://github.com/katrienantonio/hands-on-machine-learning-R-module-1


```{r, eval=FALSE, include=FALSE}
# this code can be used to extract the R code from an R Markdown (Rmd) document
library(knitr)
path <- "C:/Users/u0043788/Dropbox/Workshop AG/sheets"
setwd(path)
file.exists("ML_part1.Rmd")
purl("ML_part1.Rmd")
```


